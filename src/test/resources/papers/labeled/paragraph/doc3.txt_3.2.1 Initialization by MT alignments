
First, we use MT Alignment counts as weighting

to initialize Chinese word embeddings. In our ex-

periments, we use MT word alignments extracted

with the Berkeley Aligner (Liang et al., 2006) 1.

Specifically, we use the following equation to com-

pute starting word embeddings:

Wt-init =
Sâˆ‘

s=1

Cts + 1

Ct + S
Ws (2)

In this equation, S is the number of possible tar-

get language words that are aligned with the source

word. Cts denotes the number of times when word t

in the target and word s in the source are aligned in

the training parallel text; Ct denotes the total num-

ber of counts of word t that appeared in the target

language. Finally, Laplace smoothing is applied to

this weighting function.

1On NIST08 Zh-En training data and data from GALE MT

evaluation in the past 5 years



Single-prototype English embeddings by Huang

et al. (2012) are used to initialize Chinese em-

beddings. The initialization readily provides a set

(Align-Init) of benchmark embeddings in experi-

ments (Section 4), and ensures translation equiva-

lence in the embeddings at start of training.
