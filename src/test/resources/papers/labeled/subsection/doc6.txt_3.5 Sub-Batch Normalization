
We introduce a modification to Batch Normalization (BN) (Ioffe and Szegedy, 2015) in order
to make it applicable to target networks and decoders. Standard BN computes the mean
and variance of the activations of a given convolutional layer along the batch, height, and
width dimensions. In a decoder, the standard BN operation at training time would average

4



s0 s1 s2 s3 s4 s5

t0 t1 t2 t3 t4 t5

t1 t2 t3 t4 t5 t6

s0 s1 s2 s3 s4 s5

t0 t1 t2 t3 t4 t5

t1 t2 t3 t4 t5 t6

Figure 4: Recurrent ByteNet variants of the ByteNet architecture. Left: Recurrent ByteNet
with convolutional source network and recurrent target network. Right: Recurrent ByteNet
with bidirectional recurrent source network and recurrent target network. The latter archi-
tecture is a strict generalization of the RNN Enc-Dec network.

activations along all the tokens in the input target sequence, and the BN output for each
target token would incorporate the information about the tokens that follow it. This breaks
the conditioning structure of Eq. 1, since the succeeding tokens are yet to be predicted.

To circumvent this issue, we present Sub-Batch Normalization (SubBN). It is a variant of
BN, where a batch of training samples is split into two parts: the main batch and the
auxiliary batch. For each layer, the mean and variance of its activations are computed over
the auxiliary batch, but are used for the batch normalization of the main batch. At the
same time, the loss is computed only on the predictions of the main batch, ignoring the
predictions from the auxiliary batch.
