
In ยง1, it was claimed that NMT generates unbal-
anced outputs. To demostrate this, we have to eval-
uate the partial translations, which is not trivial (Liu
and Huang, 2014). Inspired by (Liu and Huang,
2014), we employ the idea of partial BLEU rather
than potential BLEU, as there is no future string
concept during NMT decoding. In addition, since
the lower n-gram (for example, 1-gram) is easier to
be aligned to the uncovered words in source side,

5This configuration achieved the significant improvements
over the default setting on JP-EN.

413



Systems dev test
Moses 27.9 29.4

Moses-hier 28.6 30.2
NMT-l2r 31.5 32.4
NMT-r2l 31.5 32.6
NMT-J 33.0 34.1

NMT-l2r-5 32.6 33.7
NMT-r2l-5 33.0 34.3
NMT-J-5 33.8 35.0

NMT-l2r-10 32.5 33.6
NMT-r2l-10 33.0 34.2

Table 2: BLEU comparison of the proposed model NMT-Joint
with three baselines on JP-EN task.

which might negatively affect the absolute statis-
tics of evaluation,6 we employ the partial 4-gram as
the metric to evaluate the quality of partial transla-
tions (both prefixes and suffixes). In Table 1, we
can see that the prefixes are of higher quality than
the suffixes for a single left-to-right model (NMT-
l2r). In contrast to this, it can be seen that our joint
model (NMT-J) that includes one left-to-right and
one right-to-left model, successfully addresses this
issue, producing balanced outputs.

Table 2 shows the main results on the JP-EN task.
From this table, we can see that, although a sin-
gle NMT model (either left-to-right or right-to-left)
comfortably outperforms the Moses and Moses-hier
baselines, our simple NMT-J (with one l2r and one
r2l NMT model) obtain gains of 1.5 BLEU points
over a single NMT. In addition, the more power-
ful joint model NMT-J-5, which is an ensemble of
five l2r and five r2l NMT models, gains 0.7 BLEU
points over the strongest NMT ensemble NMT-r2l-
5, i.e. an ensemble of five r2l NMT models. The en-
semble of joint models achieved considerable gains
of 5.6 and 4.8 BLEU points over the state-of-the-art
Moses and Moses-hier, respectively. To the best of
our knowlege, it is the first time that an end-to-end
neural machine translation system has achieved such
improvements on the very challenging task of JP-EN
translation.

6In training SMT (Liu and Huang, 2014), we update weights
towards higher BLEU translations and thus we care more about
the relative statistics of BLEU; but in this paper, we care more
about the absolute statistics, in order to show how severe the
problem of unbalanced outputs is.

Systems nist05 nist06 nist08
Moses 35.4 33.7 25.0

Moses-hier 35.6 33.8 25.3
NMT-l2r 34.2 34.9 27.7
NMT-r2l 34.0 34.1 26.9
NMT-J 36.8 36.9 28.5

NMT-l2r-5 37.0 37.5 28.2
NMT-r2l-5 36.9 37.1 27.3
NMT-J-5 37.5 38.9 28.8

Table 3: BLEU comparison of the proposed model NMT-Joint
with baselines on CH-EN task.

One might argue that our NMT-J-5 contained ten
NMT models in total, while the NMT-l2r-5 or NMT-
r2l-5 only used five models, and thus such a com-
parison is unfair. Therefore, we integrated ten NMT
models into the NMT-r2l-10 ensemble. In Table 2,
we can see that NMT-r2l-10 is not necessarily better
than NMT-r2l-5, which is consistent with the find-
ings reported in (Zhou et al., 2002).
