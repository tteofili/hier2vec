
This paper presented Instant-feedback SWoRD,
which was designed to increase the presence of so-
lutions in peer reviews. Evaluation results showed
that Instant-feedback SWoRD achieved high perfor-
mance in predicting solution in review comments
and in triggering instant feedback. Moreover, for
reviewers who revised their reviews after receiving
instant feedback, the number of comments with so-
lution increased. In future work, we plan to use more
data from a wider range of classes to re-train the
currently deployed prediction model. Also, a com-
prehensive comparison of our approach to studies of
similar tasks would give us insight into features and
algorithms for performance improvement.

Acknowledgments
This research is supported by NSF/1122504, and
IES/R305A120370. The larger research project (co-
led by Professors Kevin Ashley, Amanda Godley,
Diane Litman and Chris Schunn) is a collaboration
with the Learning Research & Development Center
and the School of Education. We are grateful to our
colleagues in the project.

9



References
David M Blei, Andrew Y Ng, and Michael I Jordan.

2003. Latent Dirichlet allocation. The Journal of Ma-
chine Learning Research, 3:993–1022.

Kwangsu Cho and Christian D. Schunn. 2007. Scaf-
folded Writing and Rewriting in the Discipline: A
Web-based Reciprocal Peer Review System. Comput-
ers & Education, 48(3):409–426, April.

Andrea Ernst-Gerlach and Gregory Crane. 2008. Iden-
tifying Quotations in Reference Works and Primary
Materials. In Research and Advanced Technology for
Digital Libraries, volume 5173 of Lecture Notes in
Computer Science, pages 78–87. Springer Berlin Hei-
delberg.

Sarah Gielen, Elien Peeters, Filip Dochy, Patrick
Onghena, and Katrien Struyven. 2010. Improving the
effectiveness of peer feedback for learning. Learning
and Instruction, 20(4):304 – 315. Unravelling Peer
Assessment.

Soo-Min Kim, Patrick Pantel, Tim Chklovski, and Marco
Pennacchiotti. 2006. Automatically Assessing Re-
view Helpfulness. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, EMNLP ’06, pages 423–430, Stroudsburg,
PA, USA. Association for Computational Linguistics.

Kristi Lundstrom and Wendy Baker. 2009. To give is bet-
ter than to receive: The benefits of peer review to the
reviewer’s own writing. Journal of Second Language
Writing, 18(1):30–43, March.

Melissa Nelson and Christian Schunn. 2009. The na-
ture of feedback: how different types of peer feed-
back affect writing performance. Instructional Sci-
ence, 37(4):375–401.

Huy Nguyen and Diane Litman. 2013. Identifying Lo-
calization in Peer Reviews of Argument Diagrams. In
Artificial Intelligence in Education, volume 7926 of
Lecture Notes in Computer Science, pages 91–100.
Springer Berlin Heidelberg.

Huy Nguyen and Diane Litman. 2014. Improving Peer
Feedback Prediction: The Sentence Level is Right. In
Proceedings of the Ninth Workshop on Innovative Use
of NLP for Building Educational Applications, pages
99–108, Baltimore, Maryland, June. Association for
Computational Linguistics.

Huy Nguyen, Wenting Xiong, and Diane Litman. 2014.
Classroom Evaluation of a Scaffolding Intervention
for Improving Peer Review Localization. In Intelli-
gent Tutoring Systems, volume 8474 of Lecture Notes
in Computer Science, pages 272–282. Springer Inter-
national Publishing.

Lakshmi Ramachandran and Edward F. Gehringer. 2013.
A User Study on the Automated Assessment of Re-
views. In Proceedings of the Workshops at the 16th In-

ternational Conference on Artificial Intelligence in Ed-
ucation AIED 2013, Memphis, USA, July 9-13, 2013.

Lakshmi Ramachandran and Edward F. Gehringer. 2015.
Identifying Content Patterns in Peer Reviews Using
Graph-based Cohesion. In Proceedings of the Twenty-
Eighth International Florida Artificial Intelligence Re-
search Society Conference, FLAIRS 2015, Hollywood,
Florida. May 18-20, 2015., pages 269–275.

Wenting Xiong and Diane Litman. 2010. Identifying
Problem Localization in Peer-Review Feedback. In
Intelligent Tutoring Systems, volume 6095 of Lecture
Notes in Computer Science, pages 429–431. Springer
Berlin Heidelberg.

Wenting Xiong, Diane J Litman, and Christian D Schunn.
2010. Assessing Reviewer’s Performance Based on
Mining Problem Localization in Peer-Review Data.
ERIC.

Wenting Xiong, Diane Litman, and Christian Schunn.
2012. Natural Language Processing techniques for re-
searching and improving peer feedback. Journal of
Writing Research, 4(2):155–176. Query date: 2015-
05-24.

10



Proceedings of NAACL-HLT 2016 (Demonstrations), pages 11–16,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

Farasa: A Fast and Furious Segmenter for Arabic

Ahmed Abdelali Kareem Darwish Nadir Durrani Hamdy Mubarak
Qatar Computing Research Institute

Hamad Bin Khalifa University
Doha, Qatar

{aabdelali,kdarwish,ndurrani,hmubarak}@qf.org.qa

Abstract

In this paper, we present Farasa, a fast and
accurate Arabic segmenter. Our approach
is based on SVM-rank using linear kernels.
We measure the performance of the seg-
menter in terms of accuracy and efficiency,
in two NLP tasks, namely Machine Trans-
lation (MT) and Information Retrieval (IR).
Farasa outperforms or is at par with the state-
of-the-art Arabic segmenters (Stanford and
MADAMIRA), while being more than one
order of magnitude faster.
