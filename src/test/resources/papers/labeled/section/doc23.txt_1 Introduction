
Recurrent neural network (RNN) has achieved great
successes on several structured prediction tasks
(Graves, 2013; Watanabe and Sumita, 2015; Dyer
et al., 2015), in which RNNs are required to make
a sequence of dependent predictions. One of its ad-
vantages is that an unbounded history is available to
enrich the context for the prediction at the current
time-step.

Despite its successes, recently, (Liu et al., 2016)
pointed out that the RNN suffers from a fundamental
issue of generating unbalanced outputs: that is to say
the suffixes of its outputs are typically worse than the

prefixes. This is due to the fact that later predictions
directly depend on the accuracy of previous pre-
dictions. They empirically demonstrated this issue
on two simple sequence-to-sequence learning tasks:
machine transliteration and grapheme-to-phoneme
conversion.

On the more general sequence-to-sequence learn-
ing task of machine translation (MT), neural ma-
chine translation (NMT) based on RNNs has re-
cently become an active research topic (Sutskever
et al., 2014; Bahdanau et al., 2014). Compared to
those two simple tasks, MT involves in much larger
vocabulary and frequent reordering between input
and output sequences. This makes the prediction at
each time-step far more challenging. In addition,
sequences in MT are much longer, with averaged
length of 36.7 being about 5 times longer than that
in grapheme-to-phoneme conversion. Therefore, we
believe that the history is more likely to contain in-
correct predictions and the issue of unbalanced out-
puts may be more serious. This hypothesis is sup-
ported later (see Table 1 in §4.1), by an analysis that
shows the quality of the prefixes of translation hy-
potheses is much higher than that of the suffixes.

To address this issue for NMT, in this paper we
extend the agreement model proposed in (Liu et al.,
2016) to the task of machine translation. Its key
idea is to encourage the agreement between a pair
of target-directional (left-to-right and right-to-left)
NMT models in order to produce more balanced
translations and thus improve the overall translation
quality. Our contribution is two-fold:

• We introduce a simple and general method to
address the issue of unbalanced outputs for

411



NMT (§3). This method is robust without any
extra hyperparameters to tune and is easy to im-
plement. In addition, it is general enough to be
applied on top of any of the existing RNN trans-
lation models, although it was implemented on
top of the model in (Bahdanau et al., 2014) in
this paper.

• We provide an empirical evaluation of the tech-
nique on large scale Japanese-to-English and
Chinese-to-English translation tasks. The re-
sults show our model can generate more bal-
anced translation results, and achieves substan-
tial improvements (of up to 1.4 BLEU points)
over the strongest NMT baseline (§4). With
the help of an ensemble technique, our new
end-to-end NMT gains up to 5.6 BLEU points
over phrase-based and hierarchical phrase-
based Moses (Koehn et al., 2007) systems. 1
