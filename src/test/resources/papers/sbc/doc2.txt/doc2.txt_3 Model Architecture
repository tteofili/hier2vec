Our model (see Figure 1) follows the common sequence-to-sequence learning framework [39] with attention [2].
It has three components: an encoder network, a decoder network, and an attention network. The encoder
transforms a source sentence into a list of vectors, one vector per input symbol. Given this list of vectors,
the decoder produces one symbol at a time, until the special end-of-sentence symbol (EOS) is produced.
The encoder and decoder are connected through an attention module which allows the decoder to focus on
different regions of the source sentence during the course of decoding.

For notation, we use bold lower case to denote vectors (e.g., v,oi), bold upper case to represent matrices
(e.g., U,W), cursive upper case to represent sets (e.g., V ,T ), capital letters to represent sequences (e.g. X,
Y ), and lower case to represent individual symbols in a sequence, (e.g., x1, x2).

Let (X,Y ) be a source and target sentence pair. Let X = x1, x2, x3, ..., xM be the sequence of M symbols
in the source sentence and let Y = y1, y2, y3, ..., yN be the sequence of N symbols in the target sentence. The
encoder is simply a function of the following form:

x1,x2, ...,xM = EncoderRNN(x1, x2, x3, ..., xM ) (1)

In this equation, x1,x2, ...,xM is a list of fixed size vectors. The number of members in the list is the same
as the number of symbols in the source sentence (M in this example). Using Bayes’ rule the conditional
probability of the sequence P (Y |X) can be decomposed as:

P (Y |X) = P (Y |x1,x2,x3, ...,xM)

=
N∏
i=1

P (yi|y0, y1, y2, ..., yi−1; x1,x2,x3, ...,xM)
(2)

where y0 is a special “beginning of sentence” symbol that is prepended to every target sentence.
During inference we calculate the probability of the next symbol given the source sentence encoding and

the decoded target sequence so far:

P (yi|y0, y1, y2, y3, ..., yi−1; x1,x2,x3, ...,xM) (3)

Our decoder is implemented as a combination of an RNN network and a softmax layer. The decoder RNN
network produces a hidden state yi for the next symbol to be predicted, which then goes through the softmax
layer to generate a probability distribution over candidate output symbols.

In our experiments we found that for NMT systems to achieve good accuracy, both the encoder and
decoder RNNs have to be deep enough to capture subtle irregularities in the source and target languages. This
observation is similar to previous observations that deep LSTMs significantly outperform shallow LSTMs [39].
In that work, each additional layer reduced perplexity by nearly 10%. Similar to [30], we use a deep stacked
Long Short Term Memory (LSTM) [22] network for both the encoder RNN and the decoder RNN.

Our attention module is similar to [2]. More specifically, let yi−1 be the decoder-RNN output from the
past decoding time step (in our implementation, we use the output from the bi-directional bottom decoder
layer). Attention context ai for the current time step is computed according to the following formulas:

3



Figure 1: The model architecture of GNMT, Google’s Neural Machine Translation system. On the left
is the encoder network, on the right is the decoder network, in the middle is the attention module. The
bottom encoder layer is bi-directional: the pink nodes gather information from left to right while the green
nodes gather information from right to left. The other layers of the encoder are uni-directional. Residual
connections start from the layer third from the bottom in the encoder and decoder. The model is partitioned
into multiple GPUs to speed up training. In our setup, we have 8 encoder LSTM layers (1 bi-directional layer
and 7 uni-directional layers), and 8 decoder layers. With this setting, one model replica is partitioned 8-ways
and is placed on 8 different GPUs typically belonging to one host machine. During training, the bottom
bi-directional encoder layers compute in parallel first. Once both finish, the uni-directional encoder layers
can start computing, each on a separate GPU. To retain as much parallelism as possible during running
the decoder layers, we use the bottom decoder layer output only for obtaining recurrent attention context,
which is sent directly to all the remaining decoder layers. The softmax layer is also partitioned and placed on
multiple GPUs. Depending on the output vocabulary size we either have them run on the same GPUs as the
encoder and decoder networks, or have them run on a separate set of dedicated GPUs.

st = AttentionFunction(yi−1,xt) ∀t, 1 ≤ t ≤M

pt = exp(st)/
M∑
t=1

exp(st) ∀t, 1 ≤ t ≤M

ai =
M∑
t=1

pt.xt

(4)

where AttentionFunction in our implementation is a feed forward network with one hidden layer.
