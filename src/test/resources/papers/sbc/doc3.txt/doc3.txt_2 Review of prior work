
Distributed word representations are useful in NLP

applications such as information retrieval (Paşca et

al., 2006; Manning et al., 2008), search query ex-

pansions (Jones et al., 2006), or representing se-

mantics of words (Reisinger et al., 2010). A num-

ber of methods have been explored to train and ap-

ply word embeddings using continuous models for

language. Collobert et al. (2008) learn embed-

dings in an unsupervised manner through a con-

trastive estimation technique. Mnih and Hinton (

2008), Morin and Bengio ( 2005) proposed efficient

hierarchical continuous-space models. To system-

atically compare embeddings, Turian et al. (2010)

evaluated improvements they bring to state-of-the-

art NLP benchmarks. Huang et al. (2012) intro-

duced global document context and multiple word

prototypes. Recently, morphology is explored to

learn better word representations through Recursive

Neural Networks (Luong et al., 2013).

Bilingual word representations have been ex-

plored with hand-designed vector space mod-

els (Peirsman and Padó , 2010; Sumita, 2000),

and with unsupervised algorithms such as LDA and

LSA (Boyd-Graber and Resnik, 2010; Tam et al.,

2007; Zhao and Xing, 2006). Only recently have

continuous space models been applied to machine

translation (Le et al., 2012). Despite growing in-

terest in these models, little work has been done

along the same lines to train bilingual distributioned

word represenations to improve machine translation.

In this paper, we learn bilingual word embeddings

which achieve competitive performance on seman-

tic word similarity, and apply them in a practical

phrase-based MT system.
