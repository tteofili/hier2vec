
The past several years have witnessed the rapid development of end-to-end neu-
ral machine translation (NMT) (Kalchbrenner and Blunsom2013; Sutskever et
al.2014; Bahdanau et al.2015). Unlike conventional statistical machine translation
(SMT) (Brown et al.1993; Koehn et al.2003; Chiang2007), NMT proposes to use
a single, large neural network instead of latent structures to model the translation
process. This leads to the following benefits. First, the use of distributed represen-
tations of words proves to alleviate the curse of dimensionality problem (Bengio et
al.2003). Second, there is no need to design features to capture translation regular-
ities explicitly, which is very tricky in SMT. Instead, NMT is capable of learning
representations directly from the training data. Third, Long Short-Term Memory
(Hochreiter and Schmidhuber1997) enables NMT to capture long-distance reorder-
ing, which is a notorious challenge in SMT.

∗Corresponding author: tuzhaopeng@gmail.com

1

ar
X

iv
:1

60
1.

04
81

1v
2 

 [
cs

.C
L

] 
 2

0 
Ja

n 
20

16



Figure 1: Examples of over-translation (left panel) and under-translation (right-
panel) generated by attentional NMT.

However, a serious problem with NMT is the lack of coverage. In phrase-based
SMT (Koehn et al.2003), a decoder maintains a coverage vector to indicate whether
a source word is translated or not. This is important for ensuring that each source
word is translated exactly in decoding. The decoding process is completed when
all source words are translated. In NMT, there is no such coverage vector and the
decoding process ends only when the end-of-sentence tag is produced. We believe
that lacking coverage might result in following problems in NMT:

1. Over-translation: some words are unnecessarily translated for multiple times;

2. Under-translation: some words are wrongly untranslated.

Specifically, in the state-of-the-art attentional NMT model (Bahdanau et al.2015),
generating a target word heavily depends on the relevant parts on the source side.
As each source word is involved in calculating the attention for all target words,
over-translation and under-translation inevitably happen because of the inappro-
priate imbalance of the “fertility” (i.e., the number of target words generated) of
source words. Figure 1 shows examples: the Chinese phrase “zhudao zuoyong”
is over translated to “leading role” twice (left panel), while “qunian” (means “last
year”) is wrongly untranslated (right panel).

In this work, we propose a coverage-based approach to NMT to alleviate the
over-translation and under-translation problems. Basically, we append annotation
vectors to the intermediate representation of NMT models, which is updated after

2



each attentive read during the decoding process to keep track of the attention his-
tory. Those annotation vectors, when entering into attention model, can help adjust
the future attention and significantly improve the alignment between source and
target. This design potentially contains many particular cases for coverage model-
ing with contrasting characteristics, which all share a clear linguistic intuition and
yet can be trained in a data driven fashion. Notably, in a simple and effective case,
we achieve by far the best performance by re-defining the concept of fertility, as
a successful example of re-introducing linguistic knowledge into neural network-
based NLP models. Experiments on large-scale Chinese-English datasets show
that our coverage-based NMT system outperforms conventional attentional NMT
significantly on both translation and alignment tasks.
