
While linguistically-inspired coverage in NMT is similar in spirit to that in SMT,
there is one key difference: it indicates what percentage of source words have been
translated (i.e. soft coverage). In NMT, each target word yi is generated from
all source words with probabilities αi,j for source word xj . In other words, each
source word xj involves in generating all target words and generates αi,j target
word at time step i. Note that unlike in SMT where each source word is not fully
translated at one decoding step, xj is partially translated at each decoding step in
NMT . Therefore, the coverage at time step i denotes the translated ratio of that
each source word is translated.

7



We use a scalar (d = 1) to represent linguistic coverages for each source word
and employ an accumulate operation for gupdate. We iteratively construct linguistic
coverages through an accumulation of alignment probabilities generated by the at-
tention model, each of which is normalized by a distinct context-dependent weight.
The coverage of source word xj at time step i is computed by

βi,j =
1

Φj

i∑
k=1

αk,j (9)

where Φj is a pre-defined weight which indicates the number of target words xj is
expected to generate. The simplest way is to follow Xu et al. (2015) in image-to-
caption translation to fix Φ = 1 for all source words, which means that we directly
use the sum of previous alignment probabilities without normalization as coverage
for each word, as done in (Cohn et al.2016).

However, in natural languages, different types of source words contributes dif-
ferently to the generation of translation. Take the sentence pairs in Figure 1 as
an example, the adjective on the source side “zhudao” is translated into one target
word “leading”, while the noun “jinnian” is translated into two words “this year”.
Therefore, we need to assign a distinct Φj for each source word. Ideally, we expect
Φj =

∑Ty
k=1 αk,j with Ty be the total number of time steps in decoding. How-

ever, such desired value is not available before decoding, thus is not suitable in this
scenario.

Fertility To predict Φj , we introduce the concept of fertility, which is firstly
proposed in word-level SMT (Brown et al.1993). Fertility of source word xj tells
how many target words xj produces. In SMT, the fertility is a random variable Φj ,
whose distribution p(Φj = φ) is determined by the parameters of word alignment
models (e.g. IBM models). In this work, we compute the fertility Φj by

Φj = N (xj |x) = N (hj) = N · σ(Ufhj) (10)

where N ∈ R is a predefined constantto denoting the maximum number of tar-
get words one source word can produce, σ(·) is a logistic sigmoid function, and
Uf ∈ R1×2n is the weight matrix. Here we use hj to denote (xj |x) since hj con-
tains information about the whole input sentence with a strong focus on the parts
surrounding xj (Bahdanau et al.2015). Since Φj does not depend on i, we can
pre-compute it before decoding to minimize the computational cost.

While our fertility model is similar in spirit to that in SMT, there are two
key differences which reflect how we simplify and adapt from the original model.
First, fertility in SMT is a random variable with a set of fertility probabilities,

8



Figure 5: Example translations of coverage-based NMT. Coverage model alleviates
the problems of over-translation and under-translation shown in Figure 1.

n(Φj |xj) = p(Φj−11 ,x), which depends on the fertilities of previous source words.
To simplify the calculation and adapt it to the attention model in NMT, we de-
fine the fertility in NMT as a constant number, which is independent of previous
fertilities. Second, Φj in SMT is an integer sum over binary alignment decisions
whereas that in NMT is a real sum over soft alignment probabilities.
