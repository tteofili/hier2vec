
Table 2 shows the best results obtained with our
data selection using the two neural network archi-
tectures proposed (CNN and BLSTM) and cross-
entropy method for each language pair (EN-FR,
DE-EN, EN-DE).

In EN-FR and EN-DE, FR-EN, translation qual-
ity using DS improves over bsln-emea-euro,
but using significantly less data (20%, 23% and
26% of the total amount of out-of-domain data,
respectively). In the case of DE-EN, translation
quality results are similar, but using only a 23% of
the data. According to these results, we can state
that our DS strategy is able to deliver similar qual-
ity than using all the data, but only with a rough
quarter of the data.

All proposed DS methods are mostly able to im-
prove over random selection but in some cases dif-

ferences are not significant. It should be noted that
beating random is very hard, since all DS methods,
including random, will eventually converge to the
same point: adding all the data available. The key
difference is the amount of data needed for achiev-
ing the same translation quality.

Results obtained in terms of BLEU with our DS
method are slightly better than the ones obtained
with cross-entropy. However, cross-entropy re-
quires significantly more sentences to reach com-
parable translation quality.

Lastly, CNN and BLSTM networks seem to per-
form similarly. Therefore, we conclude that both
architectures are good options for this task.
