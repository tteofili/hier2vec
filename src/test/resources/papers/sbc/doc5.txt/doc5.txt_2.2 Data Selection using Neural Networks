
In this work, we tackle the DS problem as a classi-
fication task. Let us consider a classifier model M
that assigns a probability pM (I|x) to a sentence
x, depending whether x belongs to the in-domain
corpus I or not.

In this case, to obtain the selection S, one could
just apply M to each sentence from the out-of-
domain pool G and select the most probable ones.

CNN / BLSTM

pM(I|x)

Softmax

FC

w-emb w-emb w-emb

x1 x2 x|x|

Figure 1: Gen-
eral architecture of
the proposed classi-
fier. w-emb stands
for word-embedding
and FC for fully-
connected layer.

We explored the use of
CNN and BLSTM net-
works as sentence en-
coders. As shown in
Fig. 1, the input sen-
tence is fed to our sys-
tem following a one-hot
codification scheme and
is projected to a continu-
ous space by means of a
word-embedding matrix.
Next, the input sentence
is processed either by a
CNN or a BLSTM net-
work. After this, we
add one or more fully-
connected (FC) layers.
Finally, we apply a soft-
max function, in order to obtain normalized prob-

abilities. All elements from the neural classifier
can be jointly trained by maximum likelihood.

Convolutional Neural Networks. CNNs have
proven their representation capacity, not only in
computer vision tasks (Szegedy et al., 2015), but
also representing text (Kalchbrenner and Blun-
som, 2013; Kim, 2014). In this work, we used
the non-static CNN proposed by Kim (2014).

This CNN consists in the application of a set of
filters to windows of different length. These filters
apply a non-linear function (e.g. ReLU). Next, a
max-pooling operation is applied to the set of con-
volutional filters. As result, the CNN obtains a
feature vector representing the input sentence.

Recurrent Neural Networks. In recurrent neu-
ral networks, connections form a directed cycle.
This allows the network to maintain an internal
state and be powerful sequence modelers. More-
over, bidirectional networks have two indepen-
dent recurrent layers, one processing the input se-
quence in a forward manner and other processing
it a backward manner. Therefore, they allow to
exploit the full context at each time-step. Gated
units, such as LSTM (Gers et al., 2000), mitigate
the vanishing gradient problem and hence, they are
able tp properly model long sequences.

BLSTM networks can be used for effectively
encoding a sentence by concatenating the last hid-
den state of the forward and backward LSTM lay-
ers. This provides a compact representation of the
sentence, which takes into account relationships in
both time directions.
