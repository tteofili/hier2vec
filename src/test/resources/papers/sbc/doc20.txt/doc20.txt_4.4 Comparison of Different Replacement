
Strategies

The performances of different replacement strategies are
shown in Figure 3. It can be seen that considering bilingual
context or bilingual similarity does improve the performance

2855



System 03 (dev) 04 05 06 Average

Moses 28.68 29.87 27.27 29.17 28.77
Bahdanau et al. (2015) 25.65 28.94 25.13 27.86 26.90
Luong et al. (2015) 27.63 30.02 26.42 28.72 28.20
Ours 29.85 33.08 28.95 32.31 31.05

Table 1: Translation results for different systems. Bahdanau et al. (2015) is the NMT model with attention mechanism, which
is adopted as our baseline without replacement. Luong et al. (2015) is the approach to decorate target unk with alignment
information.

 

 

24

25

26

27

28

29

30

31

32

baseline no-rep simple bi-lm bi-sim

Figure 3: Comparison of different replacement strategies.
The performance is an averaged one over all NIST data sets
we adopt.
no-rep: use original sentence without replacement
simple: use most similar word for replacement
bi-lm: use bilingual bi-directional language model to choose
from top similar words
bi-sim: use bilingual word similarity to choose from top sim-
ilar words

over the simple replacement method, although the magnitude
is not so significant when compared with the improvement of
the simple method over the baseline.

We also show the performance of translating original test-
ing sentences without replacement in the figure. The result is
quite impressive. Nearly 3 BLEU points can be achieved over
the baseline if we use the NMT model trained on the bilin-
gual data with rare words replaced, while keeping the testing
sentence unchanged. The improvement is even larger than
that brought by replacing the testing data. This demonstrates
that training on complete sentences can greatly improve the
quality of parameter estimation, and thus lead to much better
translations.
