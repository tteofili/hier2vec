
The features of our plain hierarchical phrase-based
baseline are:

• Rule translation log-probabilities in both
target-to-source and source-to-target direc-
tion, smoothed with Good-Turing discount-
ing (Foster et al., 2006).

• Lexical translation log-probabilities in both
target-to-source and source-to-target direc-
tion.

• Seven binary features indicating absolute oc-
currence count classes of translation rules
(with count classes 1, 2, 3, 4, 5-6, 7-10, >10).

• An indicator feature that fires on applications
of the glue rule.

• Word penalty.
• Rule penalty.
• A 5-gram language model.

We discard rules with non-terminals on their
right-hand side if they are singletons in the train-
ing data. The baseline language model is a lin-
ear interpolation of three 5-gram LMs trained over
the Romanian news2015, Europarl, and SETimes2
training data, respectively, with pruning of single-
ton n-grams of order three and higher.6 We run
the Moses chart-based decoder with cube prun-
ing, configured at a maximum chart span of 25 and
otherwise default settings.
