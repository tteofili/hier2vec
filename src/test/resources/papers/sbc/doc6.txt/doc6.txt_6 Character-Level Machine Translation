Russian

0

100

200

300

400

500

E
ng

lis
h

ρ = .963

Figure 5: Lengths of sentences in characters and their correlation coefficient for the En-De
and the En-Ru WMT NewsTest-2013 validation data. The correlation coefficient is similarly
high (ρ > 0.96) for all other language pairs that we inspected.

At the same time, around 3000 demonstrators attempted to reach the official residency of
Prime Minister Nawaz Sharif.

Gleichzeitig versuchten rund 3000 Demonstranten, zur Residenz von Premierminister
Nawaz Sharif zu gelangen.

Gleichzeitig haben etwa 3000 Demonstranten versucht, die offizielle Residenz des
Premierministers Nawaz Sharif zu erreichen.

Just try it: Laura, Lena, Lisa, Marie, Bettina, Emma and manager Lisa Neitzel
(from left to right) are looking forward to new members.

Einfach ausprobieren: Laura, Lena, Lisa, Marie, Bettina, Emma und Leiterin Lisa Neitzel
(von links) freuen sich auf Mitstreiter.

Probieren Sie es aus: Laura, Lena, Lisa, Marie, Bettina, Emma und Manager Lisa Neitzel
(von links nach rechts) freuen sich auf neue Mitglieder.

He could have said, “I love you,” but it was too soon for that.

Er hätte sagen können “ich liebe dich”, aber dafür war es noch zu früh.

Er hätte sagen können: “I love you”, aber es war zu früh.

Table 4: Raw output translations generated from the ByteNet that highlight interesting
reordering and transliteration phenomena. For each group, the first row is the English
source, the second row is the ground truth German target, and the third row is the ByteNet
translation.

units d is 892. The size of the kernel in the source network is 1× 5, whereas the size of the
masked kernel in the target network is 1× 3. We use bags of character n-grams as additional
embeddings at the source and target inputs: for n > 2 we prune all n-grams that occur less
than 500 times. For the optimization we use Adam with a learning rate of 0.003.

Each sentence is padded with special characters to the nearest greater multiple of 25. Each
pair of sentences is mapped to a bucket based on the pair of padded lengths for efficient
batching during training. We find that Sub-BN learns bucket-specific statistics that cannot
easily be merged across buckets. We circumvent this issue by simply searching over possible
target intervals as a first step during decoding with a beam search; each hypothesis uses
Sub-BN statistics that are specific to a target length interval. The hypotheses are ranked
according to the average likelihood of each character.

Table 3 contains the results of the experiments. We note that the lengths of the translations
generated by the ByteNet are especially close to the lengths of the reference translations and
do not tend to be too short; the brevity penalty in the BLEU scores is 0.995 and 1.0 for the
two test sets, respectively. We also note that the ByteNet architecture seems particularly
apt for machine translation. The correlation coefficient between the lengths of sentences
from different languages is often very high (Fig. 5), an aspect that is compatible with the
resolution preserving property of the architecture.

Table 4 contains some of the unaltered generated translations from the ByteNet that highlight
reordering and other phenomena such as transliteration. The character-level aspect of the
model makes post-processing unnecessary in principle. We further visualize the sensitivity of
the ByteNet’s predictions to specific source and target inputs. Figure 6 represents a heatmap
of the magnitude of the gradients of source and target inputs with respect to the generated
outputs. For visual clarity, we sum the gradients for all the characters that make up each

8



word and normalize the values along each column. In contrast with the attentional pooling
mechanism (Bahdanau et al., 2014), this general technique allows us to inspect not just
dependencies of the outputs on the source inputs, but also dependencies of the outputs on
previous target inputs, or on any other neural network layers.
