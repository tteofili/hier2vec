
In our comparison we consider the following neural translation models: the Recurrent
Continuous Translation Model (RCTM) 1 and 2 (Kalchbrenner and Blunsom, 2013); the
RNN Enc-Dec (Sutskever et al., 2014; Cho et al., 2014); the RNN Enc-Dec Att with the
attentional pooling mechanism (Bahdanau et al., 2014) of which there are a few variations
(Luong et al., 2015; Chung et al., 2016a); the Grid LSTM translation model (Kalchbrenner
et al., 2016a) that uses a multi-dimensional architecture; the Extended Neural GPU model
(Kaiser and Bengio, 2016) that has a convolutional RNN architecture; the ByteNet and the
two Recurrent ByteNet variants.

The two grounds of comparison are the desiderata (i) and (ii) set out in Sect 2.1. We separate
the computation time desideratum (i) into three columns. The first column indicates the
time complexity of the network as a function of the length of the sequences and is denoted by
Time. The other two columns NetS and NetT indicate, respectively, whether the source
and the target network uses a convolutional structure (CNN) or a recurrent one (RNN);
a CNN structure has the advantage that it can be run in parallel along the length of the
sequence. We also break the learning desideratum (ii) into three columns. The first is
denoted by RP and indicates whether the source representation in the network is resolution
preserving. The second PathS column corresponds to the length in layer steps of the shortest
path between a source token and any output target token. Similarly, the third PathT column
corresponds to the length of the shortest path between an input target token and any output
target token. Shorter paths lead to better forward and backward signal propagation.

Table 1 summarizes the properties of the models. The ByteNet, the Recurrent ByteNets and
the RNN Enc-Dec are the only networks that have linear running time (up to the constant c).
The RNN Enc-Dec, however, does not preserve the source sequence resolution, a feature that
aggravates learning for long sequences such as those in character-level machine translation
(Luong and Manning, 2016). The RCTM 2, the RNN Enc-Dec Att, the Grid LSTM and
the Extended Neural GPU do preserve the resolution, but at a cost of a quadratic running
time. The ByteNet stands out also for its Path properties. The dilated structure of the
convolutions connects any two source or target tokens in the sequences by way of a small
number of network layers corresponding to the depth of the source or target networks. For
character sequences where learning long-range dependencies is important, paths that are
sub-linear in the distance are advantageous.
