
Neural Machine Translation in Linear Time

Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan

Aäron van den Oord, Alex Graves, Koray Kavukcuoglu

{nalk,lespeholt,simonyan,avdnoord,gravesa,korayk}@google.com

Google DeepMind, London, UK

Abstract

We present a neural architecture for sequence processing. The ByteNet is
a stack of two dilated convolutional neural networks, one to encode the
source sequence and one to decode the target sequence, where the target
network unfolds dynamically to generate variable length outputs. The
ByteNet has two core properties: it runs in time that is linear in the length
of the sequences and it preserves the sequences’ temporal resolution. The
ByteNet decoder attains state-of-the-art performance on character-level
language modelling and outperforms the previous best results obtained with
recurrent neural networks. The ByteNet also achieves a performance on raw
character-level machine translation that approaches that of the best neural
translation models that run in quadratic time. The implicit structure learnt
by the ByteNet mirrors the expected alignments between the sequences.
