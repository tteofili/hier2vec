
To assess the quality of APE systems and produce
a ranking based on human judgement, as well as
analyze how humans perceive TER/BLEU perfor-
mance differences between the submitted systems,
two runs of human evaluations were conducted.
The whole evaluation took approximately a month
and was performed mainly by student translators
who annotated the APE systems’ outputs. This
subsection describes the human evaluation pro-

182



cedure, gives details about the annotators’ back-
grounds and profiles, and finally presents the re-
sults of the evaluation.
