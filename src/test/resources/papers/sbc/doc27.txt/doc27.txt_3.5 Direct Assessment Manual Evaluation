
BLEU-.8

-.6

-.4

-.2

.0

.2

.4

.6

.8

HUMAN

JHU-PBMT

ONLINE-G

ONLINE-B

CMU

TBTK-MORPH-HPB

PARFDA

JHU-HLTCOE

YSDA

ONLINE-A

Figure 4: Human evaluation scores versus BLEU scores for the Russian–English and Turkish–English language pairs
143



Czech–English

24 26 28 30 32

BLEU-.2

.0

.2

.4

.6

HUMAN

TT-NRC-NNBLEU
TT-NRC-MEANT

ONLINE-B

JHU-PBMT

TT-BLEU-MIRA

TT-BLEU-MERT
PJATK

TT-BEER-PRO

ONLINE-A

UEDIN-NMT

TT-AFRL

Finnish–English

18 20 22 24

BLEU

-1.0

-.8

-.6

-.4

-.2

.0

.2

.4

HUMAN

UH-OPUS

JHU-PBMT

ONLINE-B
ONLINE-G

PROMT-SMT

ONLINE-A

UH-FACTORED UEDIN-SYNTAX

UEDIN-PBMT

English–Finnish

10 12 14 16 18

BLEU

-.4

-.2

.0

.2

.4

HUMAN

UUT

ABUMATRAN-COMBO

ABUMATRAN-NMT

ABUMATRAN-PBSMT

AALTO

ONLINE-G

ONLINE-B

NYU-UMONTREAL

JHU-PBMT

UH-OPUS

ONLINE-A

UH-FACTORED
JHU-HLTCOE

English–Czech

0 2 4 6 8 10 12 14 16 18 20 22 24 26

BLEU

-1.4

-1.2

-1.0

-.8

-.6

-.4

-.2

.0

.2

.4

.6

HUMAN

TT-USAAR-HMM-MERT

TT-BLEU-MIRA

CU-MERGEDTREES

ONLINE-B

NYU-UMONTREAL

TT-BLEU-MERT

TT-USAAR-HMM-MIRA

UEDIN-CU-SYNTAX
TT-BEER-PRO

TT-USAAR-HARMONIC-MERT-MIRA

CU-TECTOMT

TT-AFRL2

UEDIN-NMT

TT-FJFI
TT-DCU

JHU-PBMT

TT-AFRL1

CU-TAMCHYNA CU-CHIMERA

ONLINE-A

Figure 5: Human evaluation scores versus BLEU scores for the Czech–English and Finnish–English language pairs

144



Figure 6: Direct Assessment of translation adequacy as carried out by workers on Mechanical Turk.

ency therefore provides a dimension of the assess-
ment that cannot be biased by the presence of a ref-
erence translation. For both fluency and adequacy,
the simpler monolingual assessment DA employs
also allows the sentence length restriction to be re-
moved.8

DA also aims to avoid the possible source of
bias identified in Bojar et al. (2011), introduced by
simultaneous assessment of several translations at
once, where systems for which translations were
more frequently compared to other low or high
quality outputs resulted in either an unfair advan-
tage or disadvantage for that system. We there-
fore elicit assessments of individual translations in
isolation from the output of other systems, an im-
portant criteria when aiming for absolute quality
judgments.

Large numbers of human assessments of trans-
lations for seven language pairs (cs-en, de-en, fi-
en, ro-en, ru-en, tr-en and en-ru) were collected on
Amazon’s Mechanical Turk.9 Table 7 shows over-
all numbers of translation assessments carried out.

Translations are arranged in sets of 100-
translations per HIT to ensure sufficient repeat
items per worker, before application of strict qual-
ity control measures to filter out assessments from
poorly performing workers. When an analogue (or
100-points, in practice) scale is employed, agree-

8The maximum sentence length with RR was 30 in
WMT16.

9www.mturk.com

ment cannot be measured using the conventional
Kappa coefficient, ordinarily applied to evaluation
of human assessment where judgments are dis-
crete categories or preferences. Instead, we fil-
ter human assessors by how consistently they rate
translations of known distinct quality.

A degraded version of a given original system
output translation is automatically generated by
substituting a sequence of words with a random
phrase, itself selected from elsewhere in the refer-
ence document. Together with the original out-
put, the degraded translation is known as a bad
reference translation pair. Bad reference pairs
are subsequently hidden within HITs, and provide
a mechanism for filtering out workers who are
simply not up to the task or those attempting to
game the system. Assessments of workers who do
not reliably score bad reference translations sig-
nificantly lower than corresponding genuine sys-
tem output translations are filtered out by com-
parison of scores they attribute to bad reference
pairs within HITs. More specifically, we apply a
paired Wilcoxon signed-rank test to score distri-
butions of bad reference pairs, yielding a p-value
for each worker we subsequently employ as a re-
liability estimate. Assessments of workers whose
p-value lies above the conventional 0.05 threshold
are omitted from the evaluation of systems.

Table 8 shows the number of unique workers
who evaluated MT output on Mechanical Turk via
DA for WMT16 for both fluency and adequacy,
those who met our filtering requirement by show-

145



Adequacy Fluency

Pre Quality Post Quality Ave. per Pre Quality Post Quality Ave. per
Control Control System Control Control System

cs-en 30,000 16,800 (56.0%) 2,800 16,880 6,880 (40.8%) 1,146
de-en 68,800 33,760 (49.1%) 3,376 20,480 10,400 (50.8%) 1,040
fi-en 63,040 30,080 (47.7%) 3,342 21,760 9,680 (44.5%) 1,075

ro-en 27,920 16,000 (57.3%) 2,285 18,960 8,000 (42.2%) 1,142
ru-en 64,960 37,040 (57.0%) 3,704 24,640 11,520 (46.8%) 1,152
tr-en 48,640 18,400 (37.8%) 2,044 28,000 10,640 (38.0%) 1,182

en-ru 38,160 15,920 (41.7%) 1,326 - - -

Overall 341,520 168,000 (49.2%) 2,666 130,720 57,120 (43.7%) 1,120

DA Manual Evaluation Assessments

Table 7: Numbers of system output translations evaluated on Mechanical Turk for direct assessment (DA) in WMT16, numbers
exclude quality control items.

(A) Sig. (A) & No Sig.
Diff. Diff.

All Bad Ref. Exact Rep.

Adequacy 1307 735 717 (98%)
Fluency 864 380 372 (98%)

DA Workers

Table 8: Number of unique human assessors for DA ade-
quacy and fluency on Mechanical Turk in WMT16, (A) those
whose scores for bad reference pairs were significantly dif-
ferent and numbers of unique human assessors in (A) whose
scores for exact repeat items also showed no significant dif-
ference, paired Wilcoxon signed-rank significance test was
applied in both cases.

ing a significantly lower score for bad reference
items, and the proportion of those workers who si-
multaneously showed no significant difference be-
tween scores they attributed in repeat assessment
of an identical previous translation.

In order to iron out differences in scoring strate-
gies of distinct workers, human assessment scores
for translations are standardized according to each
individual worker’s overall mean and standard de-
viation score. Subsequently, the overall score of a
given MT system participating in the shared task
simply comprises the mean (standardized) score of
its translations.

Table 9 includes mean DA fluency and ade-
quacy scores for all to-English systems participat-
ing in WMT16 translation task, while Table 10
includes results for the single out-of-English lan-
guage pair for which DA was run this year, English
to Russian. Mean standardized scores for systems
not significantly lower than that of any other par-
ticipating system, according to Wilcoxon signed-
rank test, for a given language pair, are highlighted
in bold. Although we also evaluated the fluency of

translations, mean standardized adequacy scores
should provide the primary mechanism for rank-
ing competing systems, since it is entirely possible
to achieve a high fluency score without conveying
the meaning of the source input. Fluency can be
employed as a secondary mechanism to break sys-
tems tied for adequacy or for diagnostic purposes.
Figures 7, 8 and 9 show results of combining sig-
nificance test conclusions for DA adequacy and
fluency, where any ties between systems tied for
adequacy are broken if that system outperformed
the other with respect to fluency. It should be
noted that RR provide official task results, while
DA results are investigatory and do not indicate
official translation task winners.

Finally, we compare scores of the official rank-
ing to mean standardized adequacy scores for sys-
tems evaluated with DA. Table 11 shows the Pear-
son correlation between Trueskill scores for sys-
tems evaluated by researchers with relative pref-
erence judgments (official results) and DA mean
scores collected via crowd-sourcing, showing high
levels of agreement reached overall for all lan-
guage pairs as correlations range from 0.92 to
0.997.

146



DA Adequacy DA Fluency
mean z mean raw (%) mean z mean raw (%)

cs
-e

n

UEDIN-NMT 0.207 75.4 0.499 78.7
JHU-PBMT 0.101 72.6 0.194 69.3
ONLINE-B 0.051 70.8 0.052 64.6
ONLINE-A 0.000 69.5 −0.057 61.2

PJATK −0.024 69.0 −0.014 62.8
CU-MERGEDTREES −0.503 55.8 −0.754 41.1

de
-e

n

UEDIN-NMT 0.204 75.8 0.339 77.5
ONLINE-A 0.095 72.7 0.094 70.1
ONLINE-B 0.086 72.2 0.015 68.4

UEDIN-SYNTAX 0.065 71.5 0.141 71.8
KIT 0.062 71.4 0.192 72.7

UEDIN-PBMT 0.042 70.9 0.004 68.6
JHU-PBMT 0.019 70.5 0.084 70.5
ONLINE-G 0.009 70.2 −0.067 65.3
ONLINE-F −0.204 64.0 −0.348 57.8

JHU-SYNTAX −0.261 62.4 −0.237 62.5

fi-
en

ONLINE-B 0.095 66.9 0.100 65.4
UEDIN-PBMT 0.087 66.3 0.149 66.6

ONLINE-G 0.084 66.4 0.009 62.3
UH-OPUS 0.065 65.9 0.105 65.3

PROMT-SMT −0.037 62.9 −0.093 58.8
UEDIN-SYNTAX −0.090 61.5 −0.041 60.9
UH-FACTORED −0.098 61.2 −0.020 61.1

ONLINE-A −0.126 60.6 −0.094 58.5
JHU-PBMT −0.391 52.7 −0.320 53.1

ro
-e

n

ONLINE-B 0.129 73.9 0.051 66.7
UEDIN-NMT 0.044 71.2 0.258 71.9

UEDIN-PBMT 0.025 71.0 0.028 65.6
UEDIN-SYNTAX 0.000 69.9 −0.020 64.6

ONLINE-A −0.012 69.7 −0.015 64.3
LIMSI −0.123 66.7 −0.071 62.8

JHU-PBMT −0.160 65.7 −0.187 60.2

ru
-e

n

ONLINE-G 0.115 74.2 0.100 69.9
AMU-UEDIN 0.103 73.3 0.178 72.2

ONLINE-B 0.083 72.8 0.030 67.8
NRC 0.060 72.7 0.092 69.9

PROMT-RULE-BASED 0.044 72.1 −0.102 63.8
UEDIN-NMT 0.011 71.1 0.245 74.3
ONLINE-A −0.007 70.8 0.020 66.7

AFRL-MITLL-PHRASE −0.040 70.1 0.047 68.4
AFRL-MITLL-CONTRAST −0.071 69.3 −0.020 66.5

ONLINE-F −0.322 61.8 −0.472 54.7

tr
-e

n

ONLINE-B 0.163 57.1 0.250 60.0
ONLINE-G 0.109 55.0 0.166 58.7
ONLINE-A 0.002 52.2 0.130 57.8

TBTK-SYSCOMB −0.077 49.6 0.009 53.2
PROMT-SMT −0.079 49.2 −0.057 51.4

YSDA −0.088 49.5 −0.036 52.6
JHU-PBMT −0.355 41.0 −0.416 43.1

JHU-SYNTAX −0.364 40.8 −0.307 46.4
PARFDA −0.367 40.5 −0.406 42.3

DA to-English Translation Task

Table 9: DA mean scores for WMT16 translation task participating systems for translation into English.

147



Adequacy Fluency Combined

ue
di

n.
nm

t

jh
u.

pb
m

t

on
lin

e.
B

on
lin

e.
A

P
JA

T
K

cu
.m

er
ge

dt
re

es

cu−mergedtrees

PJATK

online−A

online−B

jhu−pbmt

uedin−nmt

ue
di

n.
nm

t

jh
u.

pb
m

t

on
lin

e.
B

on
lin

e.
A

P
JA

T
K

cu
.m

er
ge

dt
re

es

cu−mergedtrees

PJATK

online−A

online−B

jhu−pbmt

uedin−nmt

ue
di

n.
nm

t

jh
u.

pb
m

t

on
lin

e.
B

on
lin

e.
A

P
JA

T
K

cu
.m

er
ge

dt
re

es

cu−mergedtrees

PJATK

online−A

online−B

jhu−pbmt

uedin−nmt

cs-en

Adequacy Fluency Combined

ue
di

n.
nm

t
on

lin
e.

A
on

lin
e.

B
ue

di
n.

sy
nt

ax K
IT

ue
di

n.
pb

m
t

jh
u.

pb
m

t
on

lin
e.

G
on

lin
e.

F
jh

u.
sy

nt
ax

jhu−syntax
online−F
online−G
jhu−pbmt
uedin−pbmt
KIT
uedin−syntax
online−B
online−A
uedin−nmt

ue
di

n.
nm

t
on

lin
e.

A
on

lin
e.

B
ue

di
n.

sy
nt

ax K
IT

ue
di

n.
pb

m
t

jh
u.

pb
m

t
on

lin
e.

G
on

lin
e.

F
jh

u.
sy

nt
ax

jhu−syntax
online−F
online−G
jhu−pbmt
uedin−pbmt
KIT
uedin−syntax
online−B
online−A
uedin−nmt

ue
di

n.
nm

t
K

IT
on

lin
e.

A
ue

di
n.

sy
nt

ax
on

lin
e.

B
jh

u.
pb

m
t

ue
di

n.
pb

m
t

on
lin

e.
G

jh
u.

sy
nt

ax
on

lin
e.

F

online−F
jhu−syntax
online−G
uedin−pbmt
jhu−pbmt
online−B
uedin−syntax
online−A
KIT
uedin−nmt

de-en

Adequacy Fluency Combined

on
lin

e.
B

ue
di

n.
pb

m
t

on
lin

e.
G

U
H

.o
pu

s
P

R
O

M
T.

S
M

T
ue

di
n.

sy
nt

ax
U

H
.fa

ct
or

ed
on

lin
e.

A
jh

u.
pb

m
t

jhu−pbmt
online−A
UH−factored
uedin−syntax
PROMT−SMT
UH−opus
online−G
uedin−pbmt
online−B

on
lin

e.
B

ue
di

n.
pb

m
t

on
lin

e.
G

U
H

.o
pu

s
P

R
O

M
T.

S
M

T
ue

di
n.

sy
nt

ax
U

H
.fa

ct
or

ed
on

lin
e.

A
jh

u.
pb

m
t

jhu−pbmt
online−A
UH−factored
uedin−syntax
PROMT−SMT
UH−opus
online−G
uedin−pbmt
online−B

on
lin

e.
B

ue
di

n.
pb

m
t

U
H

.o
pu

s
on

lin
e.

G
P

R
O

M
T.

S
M

T
ue

di
n.

sy
nt

ax
U

H
.fa

ct
or

ed
on

lin
e.

A
jh

u.
pb

m
t

jhu−pbmt
online−A
UH−factored
uedin−syntax
PROMT−SMT
online−G
UH−opus
uedin−pbmt
online−B

fi-en

Figure 7: Significance test results for pairs of systems competing in the news domain translation task (cs-en, de-en, fi-en),
where a green cell denotes a significantly higher DA adequacy or fluency score for the system in a given row over the system in
a given column, “Combined” results show overall conclusions when adequacy is primarily used to rank systems with fluency
used to break ties between systems tied with respect to adequacy.

148



Adequacy Fluency Combined

on
lin

e.
B

ue
di

n.
nm

t
ue

di
n.

pb
m

t
ue

di
n.

sy
nt

ax
on

lin
e.

A
LI

M
S

I
jh

u.
pb

m
t

jhu−pbmt
LIMSI
online−A
uedin−syntax
uedin−pbmt
uedin−nmt
online−B

on
lin

e.
B

ue
di

n.
nm

t
ue

di
n.

pb
m

t
ue

di
n.

sy
nt

ax
on

lin
e.

A
LI

M
S

I
jh

u.
pb

m
t

jhu−pbmt
LIMSI
online−A
uedin−syntax
uedin−pbmt
uedin−nmt
online−B

on
lin

e.
B

ue
di

n.
nm

t
ue

di
n.

pb
m

t
ue

di
n.

sy
nt

ax
on

lin
e.

A
LI

M
S

I
jh

u.
pb

m
t

jhu−pbmt
LIMSI
online−A
uedin−syntax
uedin−pbmt
uedin−nmt
online−B

ro-en

Adequacy Fluency Combined

on
lin

e.
G

A
M

U
.U

E
D

IN
on

lin
e.

B
N

R
C

P
R

O
M

T.
R

ul
e.

ba
se

d
ue

di
n.

nm
t

on
lin

e.
A

A
F

R
L.

M
IT

LL
.P

hr
as

e
A

F
R

L.
M

IT
LL

.c
on

tr
as

t
on

lin
e.

F

online−F
AFRL−MITLL−contrast
AFRL−MITLL−Phrase
online−A
uedin−nmt
PROMT−Rule−based
NRC
online−B
AMU−UEDIN
online−G

on
lin

e.
G

A
M

U
.U

E
D

IN
on

lin
e.

B
N

R
C

P
R

O
M

T.
R

ul
e.

ba
se

d
ue

di
n.

nm
t

on
lin

e.
A

A
F

R
L.

M
IT

LL
.P

hr
as

e
A

F
R

L.
M

IT
LL

.c
on

tr
as

t
on

lin
e.

F

online−F
AFRL−MITLL−contrast
AFRL−MITLL−Phrase
online−A
uedin−nmt
PROMT−Rule−based
NRC
online−B
AMU−UEDIN
online−G

A
M

U
.U

E
D

IN
on

lin
e.

G
on

lin
e.

B
N

R
C

ue
di

n.
nm

t
P

R
O

M
T.

R
ul

e.
ba

se
d

on
lin

e.
A

A
F

R
L.

M
IT

LL
.P

hr
as

e
A

F
R

L.
M

IT
LL

.c
on

tr
as

t
on

lin
e.

F

online−F
AFRL−MITLL−contrast
AFRL−MITLL−Phrase
online−A
PROMT−Rule−based
uedin−nmt
NRC
online−B
online−G
AMU−UEDIN

ru-en

Adequacy Fluency Combined

on
lin

e.
B

on
lin

e.
G

on
lin

e.
A

tb
tk

.s
ys

co
m

b
P

R
O

M
T.

S
M

T
Y

S
D

A
jh

u.
pb

m
t

jh
u.

sy
nt

ax
P

ar
F

D
A

ParFDA
jhu−syntax
jhu−pbmt
YSDA
PROMT−SMT
tbtk−syscomb
online−A
online−G
online−B

on
lin

e.
B

on
lin

e.
G

on
lin

e.
A

tb
tk

.s
ys

co
m

b
P

R
O

M
T.

S
M

T
Y

S
D

A
jh

u.
pb

m
t

jh
u.

sy
nt

ax
P

ar
F

D
A

ParFDA
jhu−syntax
jhu−pbmt
YSDA
PROMT−SMT
tbtk−syscomb
online−A
online−G
online−B

on
lin

e.
B

on
lin

e.
G

on
lin

e.
A

tb
tk

.s
ys

co
m

b
P

R
O

M
T.

S
M

T
Y

S
D

A
jh

u.
sy

nt
ax

jh
u.

pb
m

t
P

ar
F

D
A

ParFDA
jhu−pbmt
jhu−syntax
YSDA
PROMT−SMT
tbtk−syscomb
online−A
online−G
online−B

tr-en

Figure 8: Significance test results for pairs of systems competing in the news domain translation task (ro-en, ru-en, tr-en),
where a green cell denotes a significantly higher DA adequacy or fluency score for the system in a given row over the system in
a given column, “Combined” results show overall conclusions when adequacy is primarily used to rank systems with fluency
used to break ties between systems tied with respect to adequacy.

149



Adequacy

mean mean
z raw (%)

PROMT-RULE-BASED 0.258 69.0
ONLINE-G 0.101 63.8
ONLINE-B 0.092 62.5

AMU-UEDIN 0.084 63.4
UEDIN-NMT 0.062 63.2

ONLINE-A −0.008 60.8
JHU-PBMT −0.023 58.6

NYU-UMONTREAL −0.042 58.3
LIMSI −0.072 58.9

AFRL-MITLL-PHRASE −0.077 58.3
AFRL-MITLL-VERB-ANN −0.093 57.8

ONLINE-F −0.489 43.7

DA English to Russian

Table 10: DA mean scores for WMT16 translation task par-
ticipating systems for translation from English into Russian.

P
R

O
M

T.
R

ul
e.

ba
se

d
on

lin
e.

G
on

lin
e.

B
A

M
U

.U
E

D
IN

ue
di

n.
nm

t
on

lin
e.

A
jh

u.
pb

m
t

N
Y

U
.U

M
on

tr
ea

l
LI

M
S

I
A

F
R

L.
M

IT
LL

.p
hr

as
e.

ba
se

d
A

F
R

L.
M

IT
LL

.v
er

b.
an

no
t

on
lin

e.
F

online−F
AFRL−MITLL−verb−annot
AFRL−MITLL−phrase−based
LIMSI
NYU−UMontreal
jhu−pbmt
online−A
uedin−nmt
AMU−UEDIN
online−B
online−G
PROMT−Rule−based

Figure 9: Significance test results for pairs of systems com-
peting in the news domain translation task (en-ru), where a
green cell denotes a significantly higher DA adequacy score
for the system in a given row over the system in a given col-
umn.

cs-en 0.997
fi-en 0.996
tr-en 0.988

de-en 0.964
ru-en 0.961
ro-en 0.920
en-ru 0.975

DA Correlation with RR

Table 11: Correlation between overall DA standardized
mean adequacy scores and RR Trueskill scores.

150


