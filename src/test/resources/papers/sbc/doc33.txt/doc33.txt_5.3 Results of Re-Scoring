
The experimental results of re-scoring are shown
in table 2, we compare our re-scoring methods
based on the candidates generated by our best mul-
timodal NMT modal (model 3). The second row
is the results using LSTM monolingual language
model with hidden size as 300. The reason why we
can barely achieve improvement might be that the
grammar in the caption task is much easier com-
pared to other translation tasks such as dialog or
News; therefore, the candidate sentences with low
score of evaluation (BLEU or METEOR) may also
looks like a sentence, but without relevance to the
source sentence.

The third row shows the re-scoring results with
the bi-lingual autoencoder. This approach results
in drops in both BLEU and METEOR. The rea-
son might be that the quality and quantity of our
Bi-lingual corpus is insufficient for the purpose of
learning a good autoencoder. Furthermore, we ob-
serve the test perplexity is higher than the training
and validation perplexity, showing the over-fitting

Table 2: Results of re-scoring using monolin-
gual LSTM, Bi-lingual auto-encoder, and dictio-
nary based on multimodal NMT results.

BLEU METEOR
Original Model 3 36.5 (0.8) 54.1 (0.7)
Language model 36.3 (0.8) 53.3 (0.6)

Bilingual autoencoder 35.9 (0.8) 53.4 (0.7)
Bilingual dictionary 35.7 (0.8) 55.2 (0.6)

in language modeling and the effects of unknown
words. It’s clear that more investigation is required
for designing a better bilingual autoencoder for re-
scoring.

The last row shows the results using the bilin-
gual dictionary. For each word in the source sen-
tence and the target candidates, we retrieve the
term and the translation in the other language, and
count the number of matching. We can achieve
much more improvement on METEOR compared
to other methods. This is because that the qual-
ity of the translation of captions depends on how
much we correctly translate the objects and their
modifiers. The bad translation can still achieve fair
performance without re-scoring because the sen-
tence structure is similar to good translation. For
example, a lot of sentences start with “A man” and
both good and bad translation can also translate
the sentences start with “Ein Mann”. The bilingual
dictionary is proved to be an efficient re-scoring
approach to distinguish these cases.
