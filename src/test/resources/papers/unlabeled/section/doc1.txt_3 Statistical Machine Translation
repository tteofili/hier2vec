
In a commonly used statistical machine translation
system (SMT), the goal of the system (decoder,
specifically) is to find a translation f given a source
sentence e, which maximizes

p(f | e) ∝ p(e | f)p(f),

where the first term at the right hand side is called
translation model and the latter language model
(see, e.g., (Koehn, 2005)). In practice, however,
most SMT systems model log p(f | e) as a log-
linear model with additional features and corre-



sponding weights:

log p(f | e) =
N∑

n=1

wnfn(f , e) + logZ(e), (9)

where fn and wn are the n-th feature and weight,
respectively. Z(e) is a normalization constant that
does not depend on the weights. The weights are
often optimized to maximize the BLEU score on a
development set.

In the phrase-based SMT framework
introduced in (Koehn et al., 2003) and
(Marcu and Wong, 2002), the translation model
log p(e | f) is factorized into the translation
probabilities of matching phrases in the source
and target sentences.2 These probabilities are
once again considered additional features in the
log-linear model (see Eq. (9)) and are weighted
accordingly to maximize the BLEU score.

Since the neural net language model was pro-
posed in (Bengio et al., 2003), neural networks
have been used widely in SMT systems. In
many cases, neural networks have been used to
rescore translation hypotheses (n-best lists) (see,
e.g., (Schwenk et al., 2006)). Recently, however,
there has been interest in training neural networks
to score the translated sentence (or phrase pairs)
using a representation of the source sentence as
an additional input. See, e.g., (Schwenk, 2012),
(Son et al., 2012) and (Zou et al., 2013).
