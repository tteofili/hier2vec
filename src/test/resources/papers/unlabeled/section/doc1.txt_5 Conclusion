
In this paper, we proposed a new neural network
architecture, called an RNN Encoder–Decoder
that is able to learn the mapping from a sequence



of an arbitrary length to another sequence, possi-
bly from a different set, of an arbitrary length. The
proposed RNN Encoder–Decoder is able to either
score a pair of sequences (in terms of a conditional
probability) or generate a target sequence given a
source sequence. Along with the new architecture,
we proposed a novel hidden unit that includes a re-
set gate and an update gate that adaptively control
how much each hidden unit remembers or forgets
while reading/generating a sequence.

We evaluated the proposed model with the task
of statistical machine translation, where we used
the RNN Encoder–Decoder to score each phrase
pair in the phrase table. Qualitatively, we were
able to show that the new model is able to cap-
ture linguistic regularities in the phrase pairs well
and also that the RNN Encoder–Decoder is able to
propose well-formed target phrases.

The scores by the RNN Encoder–Decoder were
found to improve the overall translation perfor-
mance in terms of BLEU scores. Also, we
found that the contribution by the RNN Encoder–
Decoder is rather orthogonal to the existing ap-
proach of using neural networks in the SMT sys-
tem, so that we can improve further the perfor-
mance by using, for instance, the RNN Encoder–
Decoder and the neural net language model to-
gether.

Our qualitative analysis of the trained model
shows that it indeed captures the linguistic regu-
larities in multiple levels i.e. at the word level as
well as phrase level. This suggests that there may
be more natural language related applications that
may benefit from the proposed RNN Encoder–
Decoder.

The proposed architecture has large potential
for further improvement and analysis. One ap-
proach that was not investigated here is to re-
place the whole, or a part of the phrase table by
letting the RNN Encoder–Decoder propose target
phrases. Also, noting that the proposed model is
not limited to being used with written language,
it will be an important future research to apply the
proposed architecture to other applications such as
speech transcription.

Acknowledgments

KC, BM, CG, DB and YB would like to thank
NSERC, Calcul Québec, Compute Canada, the
Canada Research Chairs and CIFAR. FB and HS
were partially funded by the European Commis-

sion under the project MateCat, and by DARPA
under the BOLT project.

References
[Auli et al.2013] Michael Auli, Michel Galley, Chris

Quirk, and Geoffrey Zweig. 2013. Joint language
and translation modeling with recurrent neural net-
works. In Proceedings of the ACL Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1044–1054.

[Axelrod et al.2011] Amittai Axelrod, Xiaodong He,
and Jianfeng Gao. 2011. Domain adaptation via
pseudo in-domain data selection. In Proceedings of
the ACL Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 355–362.

[Bastien et al.2012] Frédéric Bastien, Pascal Lamblin,
Razvan Pascanu, James Bergstra, Ian J. Goodfellow,
Arnaud Bergeron, Nicolas Bouchard, and Yoshua
Bengio. 2012. Theano: new features and speed im-
provements. Deep Learning and Unsupervised Fea-
ture Learning NIPS 2012 Workshop.

[Bengio et al.2003] Yoshua Bengio, Réjean Ducharme,
Pascal Vincent, and Christian Janvin. 2003. A neu-
ral probabilistic language model. J. Mach. Learn.
Res., 3:1137–1155, March.

[Bengio et al.2013] Y. Bengio, N. Boulanger-
Lewandowski, and R. Pascanu. 2013. Advances
in optimizing recurrent networks. In Proceedings
of the 38th International Conference on Acoustics,
Speech, and Signal Processing (ICASSP 2013),
May.

[Bergstra et al.2010] James Bergstra, Olivier Breuleux,
Frédéric Bastien, Pascal Lamblin, Razvan Pascanu,
Guillaume Desjardins, Joseph Turian, David Warde-
Farley, and Yoshua Bengio. 2010. Theano: a CPU
and GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference
(SciPy), June. Oral Presentation.

[Chandar et al.2014] Sarath Chandar, Stanislas Lauly,
Hugo Larochelle, Mitesh Khapra, Balaraman Ravin-
dran, Vikas Raykar, and Amrita Saha. 2014. An au-
toencoder approach to learning bilingual word repre-
sentations. arXiv:1402.1454 [cs.CL], Febru-
ary.

[Dahl et al.2012] George E. Dahl, Dong Yu, Li Deng,
and Alex Acero. 2012. Context-dependent pre-
trained deep neural networks for large vocabulary
speech recognition. IEEE Transactions on Audio,
Speech, and Language Processing, 20(1):33–42.

[Devlin et al.2014] Jacob Devlin, Rabih Zbib,
Zhongqiang Huang, Thomas Lamar, Richard
Schwartz, , and John Makhoul. 2014. Fast and
robust neural network joint models for statistical
machine translation. In Proceedings of the ACL
2014 Conference, ACL ’14, pages 1370–1380.



[Gao et al.2013] Jianfeng Gao, Xiaodong He, Wen tau
Yih, and Li Deng. 2013. Learning semantic repre-
sentations for the phrase translation model. Techni-
cal report, Microsoft Research.

[Glorot et al.2011] X. Glorot, A. Bordes, and Y. Ben-
gio. 2011. Deep sparse rectifier neural networks. In
AISTATS’2011.

[Goodfellow et al.2013] Ian J. Goodfellow, David
Warde-Farley, Mehdi Mirza, Aaron Courville, and
Yoshua Bengio. 2013. Maxout networks. In
ICML’2013.

[Graves2012] Alex Graves. 2012. Supervised Se-
quence Labelling with Recurrent Neural Networks.
Studies in Computational Intelligence. Springer.

[Hochreiter and Schmidhuber1997] S. Hochreiter and
J. Schmidhuber. 1997. Long short-term memory.
Neural Computation, 9(8):1735–1780.

[Kalchbrenner and Blunsom2013] Nal Kalchbrenner
and Phil Blunsom. 2013. Two recurrent continuous
translation models. In Proceedings of the ACL Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1700–1709.

[Koehn et al.2003] Philipp Koehn, Franz Josef Och,
and Daniel Marcu. 2003. Statistical phrase-based
translation. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology - Volume 1, NAACL ’03, pages 48–54.

[Koehn2005] P. Koehn. 2005. Europarl: A parallel cor-
pus for statistical machine translation. In Machine
Translation Summit X, pages 79–86, Phuket, Thai-
land.

[Krizhevsky et al.2012] Alex Krizhevsky, Ilya
Sutskever, and Geoffrey Hinton. 2012. Ima-
geNet classification with deep convolutional neural
networks. In Advances in Neural Information
Processing Systems 25 (NIPS’2012).

[Marcu and Wong2002] Daniel Marcu and William
Wong. 2002. A phrase-based, joint probability
model for statistical machine translation. In Pro-
ceedings of the ACL-02 Conference on Empirical
Methods in Natural Language Processing - Volume
10, EMNLP ’02, pages 133–139.

[Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever,
Kai Chen, Greg Corrado, and Jeff Dean. 2013. Dis-
tributed representations of words and phrases and
their compositionality. In Advances in Neural Infor-
mation Processing Systems 26, pages 3111–3119.

[Moore and Lewis2010] Robert C. Moore and William
Lewis. 2010. Intelligent selection of language
model training data. In Proceedings of the ACL
2010 Conference Short Papers, ACLShort ’10,
pages 220–224, Stroudsburg, PA, USA.

[Pascanu et al.2014] R. Pascanu, C. Gulcehre, K. Cho,
and Y. Bengio. 2014. How to construct deep recur-
rent neural networks. In Proceedings of the Second
International Conference on Learning Representa-
tions (ICLR 2014), April.

[Saxe et al.2014] Andrew M. Saxe, James L. McClel-
land, and Surya Ganguli. 2014. Exact solutions
to the nonlinear dynamics of learning in deep lin-
ear neural networks. In Proceedings of the Second
International Conference on Learning Representa-
tions (ICLR 2014), April.

[Schwenk et al.2006] Holger Schwenk, Marta R. Costa-
Jussà, and José A. R. Fonollosa. 2006. Continuous
space language models for the iwslt 2006 task. In
IWSLT, pages 166–173.

[Schwenk2007] Holger Schwenk. 2007. Continuous
space language models. Comput. Speech Lang.,
21(3):492–518, July.

[Schwenk2012] Holger Schwenk. 2012. Continuous
space translation models for phrase-based statisti-
cal machine translation. In Martin Kay and Chris-
tian Boitet, editors, Proceedings of the 24th Inter-
national Conference on Computational Linguistics
(COLIN), pages 1071–1080.

[Socher et al.2011] Richard Socher, Eric H. Huang, Jef-
frey Pennington, Andrew Y. Ng, and Christopher D.
Manning. 2011. Dynamic pooling and unfolding
recursive autoencoders for paraphrase detection. In
Advances in Neural Information Processing Systems
24.

[Son et al.2012] Le Hai Son, Alexandre Allauzen, and
François Yvon. 2012. Continuous space transla-
tion models with neural networks. In Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, NAACL HLT ’12,
pages 39–48, Stroudsburg, PA, USA.

[van der Maaten2013] Laurens van der Maaten. 2013.
Barnes-hut-sne. In Proceedings of the First Inter-
national Conference on Learning Representations
(ICLR 2013), May.

[Vaswani et al.2013] Ashish Vaswani, Yinggong Zhao,
Victoria Fossum, and David Chiang. 2013. De-
coding with large-scale neural language models im-
proves translation. Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1387–1392.

[Zeiler2012] Matthew D. Zeiler. 2012. ADADELTA:
an adaptive learning rate method. Technical report,
arXiv 1212.5701.

[Zou et al.2013] Will Y. Zou, Richard Socher,
Daniel M. Cer, and Christopher D. Manning.
2013. Bilingual word embeddings for phrase-based
machine translation. In Proceedings of the ACL
Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1393–1398.



A RNN Encoder–Decoder

In this document, we describe in detail the architecture of the RNN Encoder–Decoder used in the exper-
iments.

Let us denote an source phrase by X = (x1,x2, . . . ,xN ) and a target phrase by Y =
(y1,y2, . . . ,yM ). Each phrase is a sequence of K-dimensional one-hot vectors, such that only one
element of the vector is 1 and all the others are 0. The index of the active (1) element indicates the word
represented by the vector.

A.1 Encoder
Each word of the source phrase is embedded in a 500-dimensional vector space: e(xi) ∈ R500. e(x) is
used in Sec. 4.4 to visualize the words.

The hidden state of an encoder consists of 1000 hidden units, and each one of them at time t is
computed by

h
〈t〉
j = zjh

〈t−1〉
j + (1− zj)h̃

〈t〉
j ,

where

h̃
〈t〉
j =tanh

(
[We(xt)]j +

[
U
(
r� h〈t−1〉

)]
j

)
,

zj =σ
(
[Wze(xt)]j +

[
Uzh〈t−1〉

]
j

)
,

rj =σ
(
[Wre(xt)]j +

[
Urh〈t−1〉

]
j

)
.

σ and � are a logistic sigmoid function and an element-wise multiplication, respectively. To make the
equations uncluttered, we omit biases. The initial hidden state h〈0〉j is fixed to 0.

Once the hidden state at the N step (the end of the source phrase) is computed, the representation of
the source phrase c is

c = tanh
(
Vh〈N〉

)
.

A.1.1 Decoder
The decoder starts by initializing the hidden state with

h′
〈0〉

= tanh
(
V′c

)
,

where we will use ·′ to distinguish parameters of the decoder from those of the encoder.
The hidden state at time t of the decoder is computed by

h′
〈t〉
j = z

′
jh
′〈t−1〉
j + (1− z

′
j)h̃′

〈t〉
j ,

where

h̃′
〈t〉
j =tanh

([
W′e(yt−1)

]
j
+ r′j

[
U′h′〈t−1〉 +Cc

])
,

z′j =σ
([

W′ze(yt−1)
]
j
+
[
U′zh

′
〈t−1〉

]
j
+ [Czc]j

)
,

r′j =σ
([

W′re(yt−1)
]
j
+
[
U′rh

′
〈t−1〉

]
j
+ [Crc]j

)
,

and e(y0) is an all-zero vector. Similarly to the case of the encoder, e(y) is an embedding of a target
word.

Unlike the encoder which simply encodes the source phrase, the decoder is learned to generate a target
phrase. At each time t, the decoder computes the probability of generating j-th word by

p(yt,j = 1 | yt−1, . . . ,y1, X) =
exp

(
gjs〈t〉

)∑K
j′=1 exp

(
gj′s〈t〉

) ,



where the i-element of s〈t〉 is

s
〈t〉
i = max

{
s′
〈t〉
2i−1, s

′〈t〉
2i

}
and

s′
〈t〉

= Ohh
′〈t〉 +Oyyt−1 +Occ.

In short, the s〈t〉i is a so-called maxout unit.
For the computational efficiency, instead of a single-matrix output weight G, we use a product of two

matrices such that

G = GlGr,

where Gl ∈ RK×500 and Gr ∈ R500×1000.

B Word and Phrase Representations

Here, we show enlarged plots of the word and phrase representations in Figs. 4–5.



Figure 6: 2–D embedding of the learned word representation. The top left one shows the full embedding space, while the other three figures show the zoomed-in
view of specific regions (color–coded).



Figure 7: 2–D embedding of the learned phrase representation. The top left one shows the full representation space (1000 randomly selected points), while the
other three figures show the zoomed-in view of specific regions (color–coded).


	1 Introduction
	2 RNN Encoder–Decoder
	2.1 Preliminary: Recurrent Neural Networks
	2.2 RNN Encoder–Decoder
	2.3 Hidden Unit that Adaptively Remembers and Forgets

	3 Statistical Machine Translation
	3.1 Scoring Phrase Pairs with RNN Encoder–Decoder
	3.2 Related Approaches: Neural Networks in Machine Translation

	4 Experiments
	4.1 Data and Baseline System
	4.1.1 RNN Encoder–Decoder
	4.1.2 Neural Language Model

	4.2 Quantitative Analysis
	4.3 Qualitative Analysis
	4.4 Word and Phrase Representations

	5 Conclusion
	A RNN Encoder–Decoder
	A.1 Encoder
	A.1.1 Decoder


	B Word and Phrase Representations

