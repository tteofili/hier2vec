
Since the proposed RNN Encoder–Decoder is not
specifically designed only for the task of machine
translation, here we briefly look at the properties
of the trained model.

It has been known for some time that
continuous space language models using
neural networks are able to learn seman-
tically meaningful embeddings (See, e.g.,
(Bengio et al., 2003; Mikolov et al., 2013)). Since
the proposed RNN Encoder–Decoder also projects
to and maps back from a sequence of words into
a continuous space vector, we expect to see a
similar property with the proposed model as well.

The left plot in Fig. 4 shows the 2–D embedding
of the words using the word embedding matrix
learned by the RNN Encoder–Decoder. The pro-
jection was done by the recently proposed Barnes-
Hut-SNE (van der Maaten, 2013). We can clearly
see that semantically similar words are clustered

with each other (see the zoomed-in plots in Fig. 4).
The proposed RNN Encoder–Decoder naturally

generates a continuous-space representation of a
phrase. The representation (c in Fig. 1) in this
case is a 1000-dimensional vector. Similarly to the
word representations, we visualize the representa-
tions of the phrases that consists of four or more
words using the Barnes-Hut-SNE in Fig. 5.

From the visualization, it is clear that the RNN
Encoder–Decoder captures both semantic and syn-
tactic structures of the phrases. For instance, in
the bottom-left plot, most of the phrases are about
the duration of time, while those phrases that are
syntactically similar are clustered together. The
bottom-right plot shows the cluster of phrases that
are semantically similar (countries or regions). On
the other hand, the top-right plot shows the phrases
that are syntactically similar.
