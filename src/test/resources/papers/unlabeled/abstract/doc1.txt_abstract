
Learning Phrase Representations using RNN Encoder–Decoder
for Statistical Machine Translation

Kyunghyun Cho
Bart van Merriënboer Caglar Gulcehre

Université de Montréal
firstname.lastname@umontreal.ca

Dzmitry Bahdanau
Jacobs University, Germany

d.bahdanau@jacobs-university.de

Fethi Bougares Holger Schwenk
Université du Maine, France

firstname.lastname@lium.univ-lemans.fr

Yoshua Bengio
Université de Montréal, CIFAR Senior Fellow

find.me@on.the.web

Abstract

In this paper, we propose a novel neu-
ral network model called RNN Encoder–
Decoder that consists of two recurrent
neural networks (RNN). One RNN en-
codes a sequence of symbols into a fixed-
length vector representation, and the other
decodes the representation into another se-
quence of symbols. The encoder and de-
coder of the proposed model are jointly
trained to maximize the conditional prob-
ability of a target sequence given a source
sequence. The performance of a statisti-
cal machine translation system is empiri-
cally found to improve by using the con-
ditional probabilities of phrase pairs com-
puted by the RNN Encoder–Decoder as an
additional feature in the existing log-linear
model. Qualitatively, we show that the
proposed model learns a semantically and
syntactically meaningful representation of
linguistic phrases.
