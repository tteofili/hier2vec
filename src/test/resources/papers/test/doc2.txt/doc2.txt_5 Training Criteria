
Given a dataset of parallel text containing N input-output sequence pairs, denoted D ≡
{

(X(i), Y ∗(i))
}N
i=1,

standard maximum-likelihood training aims at maximizing the sum of log probabilities of the ground-truth
outputs given the corresponding inputs,

OML(θ) =
N∑
i=1

logPθ(Y ∗(i) | X(i)) . (7)

The main problem with this objective is that it does not reflect the task reward function as measured by the
BLEU score in translation. Further, this objective does not explicitly encourage a ranking among incorrect
output sequences – where outputs with higher BLEU scores should still obtain higher probabilities under the
model – since incorrect outputs are never observed during training. In other words, using maximum-likelihood
training only, the model will not learn to be robust to errors made during decoding since they are never
observed, which is quite a mismatch between the training and testing procedure.

Several recent papers [33, 38, 31] have considered different ways of incorporating the task reward into
optimization of neural sequence-to-sequence models. In this work, we also attempt to refine a model pre-
trained on the maximum likelihood objective to directly optimize for the task reward. We show that, even on
large datasets, refinement of state-of-the-art maximum-likelihood models using task reward improves the
results considerably.

We consider model refinement using the expected reward objective (also used in [33]), which can be
expressed as

ORL(θ) =
N∑
i=1

∑
Y ∈Y

Pθ(Y | X(i)) r(Y, Y ∗(i)). (8)

Here, r(Y, Y ∗(i)) denotes the per-sentence score, and we are computing an expectation over all of the output
sentences Y , up to a certain length.

The BLEU score has some undesirable properties when used for single sentences, as it was designed to
be a corpus measure. We therefore use a slightly different score for our RL experiments which we call the
“GLEU score”. For the GLEU score, we record all sub-sequences of 1, 2, 3 or 4 tokens in output and target
sequence (n-grams). We then compute a recall, which is the ratio of the number of matching n-grams to
the number of total n-grams in the target (ground truth) sequence, and a precision, which is the ratio of
the number of matching n-grams to the number of total n-grams in the generated output sequence. Then
GLEU score is simply the minimum of recall and precision. This GLEU score’s range is always between 0
(no matches) and 1 (all match) and it is symmetrical when switching output and target. According to our
experiments, GLEU score correlates quite well with the BLEU metric on a corpus level but does not have its
drawbacks for our per sentence reward objective.

As is common practice in reinforcement learning, we subtract the mean reward from r(Y, Y ∗(i)) in equation
8. The mean is estimated to be the sample mean of m sequences drawn independently from distribution
Pθ(Y | X(i)). In our implementation, m is set to be 15. To further stabilize training, we optimize a linear

8



combination of ML (equation 7) and RL (equation 8) objectives as follows:

OMixed(θ) = α ∗ OML(θ) +ORL(θ) (9)

α in our implementation is typically set to be 0.25.
In our setup, we first train a model using the maximum likelihood objective (equation 7) until convergence.

We then refine this model using a mixed maximum likelihood and expected reward objective (equation 9),
until BLEU score on a development set is no longer improving. The second step is optional.
