We evaluate our models using the standard BLEU score metric. To be comparable to previous work [39, 30, 43],
we report tokenized BLEU score as computed by the multi-bleu.pl script, downloaded from the public
implementation of Moses (on Github), which is also used in [30].

As is well-known, BLEU score does not fully capture the quality of a translation. For that reason we also
carry out side-by-side (SxS) evaluations where we have human raters evaluate and compare the quality of
two translations presented side by side for a given source sentence. Side-by-side scores range from 0 to 6,
with a score of 0 meaning “completely nonsense translation”, and a score of 6 meaning “perfect translation:
the meaning of the translation is completely consistent with the source, and the grammar is correct”. A
translation is given a score of 4 if “the sentence retains most of the meaning of the source sentence, but may
have some grammar mistakes”, and a translation is given a score of 2 if “the sentence preserves some of the
meaning of the source sentence but misses significant parts”. These scores are generated by human raters
who are fluent in both languages and hence often capture translation quality better than BLEU scores.
