
The RNN Encoder–Decoder used in the experi-
ment had 1000 hidden units with the proposed
gates at the encoder and at the decoder. The in-
put matrix between each input symbol x〈t〉 and the
hidden unit is approximated with two lower-rank
matrices, and the output matrix is approximated



Models
BLEU

dev test
Baseline 30.64 33.30
RNN 31.20 33.87
CSLM + RNN 31.48 34.64
CSLM + RNN + WP 31.50 34.54

Table 1: BLEU scores computed on the develop-
ment and test sets using different combinations of
approaches. WP denotes a word penalty, where
we penalizes the number of unknown words to
neural networks.

similarly. We used rank-100 matrices, equivalent
to learning an embedding of dimension 100 for
each word. The activation function used for h̃ in
Eq. (8) is a hyperbolic tangent function. The com-
putation from the hidden state in the decoder to
the output is implemented as a deep neural net-
work (Pascanu et al., 2014) with a single interme-
diate layer having 500 maxout units each pooling
2 inputs (Goodfellow et al., 2013).

All the weight parameters in the RNN Encoder–
Decoder were initialized by sampling from an
isotropic zero-mean (white) Gaussian distribution
with its standard deviation fixed to 0.01, except
for the recurrent weight parameters. For the re-
current weight matrices, we first sampled from a
white Gaussian distribution and used its left singu-
lar vectors matrix, following (Saxe et al., 2014).

We used Adadelta and stochastic gradient
descent to train the RNN Encoder–Decoder
with hyperparameters � = 10−6 and ρ =
0.95 (Zeiler, 2012). At each update, we used 64
randomly selected phrase pairs from a phrase ta-
ble (which was created from 348M words). The
model was trained for approximately three days.

Details of the architecture used in the experi-
ments are explained in more depth in the supple-
mentary material.
