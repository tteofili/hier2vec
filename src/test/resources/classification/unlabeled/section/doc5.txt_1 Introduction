
The performance of a SMT system is dependent
on the quantity and quality of the available training
data. Typically, SMT systems are trained with all
available data, assuming that the more data used
to train the system, the better. This assumption is
backed by the evidence that scaling to ever larger
data shows continued improvements in quality,
even when one trains models over billions of n-
grams (Brants et al., 2007). In the SMT context, n-
grams refers to sequences of n consecutive words.
However, growing the amount of data available is
only feasible to a certain extent. Moreover so,
whenever it is critical that such data is related to
the task at hand. In fact, translation quality is neg-
atively affected when there is insufficient training
data for the specific domain to be tackled in pro-
duction conditions (Callison-Burch et al., 2007;
Koehn, 2010).

Data selection (DS) has the aim of selecting for
training, the best subset of sentence pairs from
an available pool, so that the translation quality
achieved in the target domain is improved.

State-of-the-art DS approaches rely on the idea
of choosing those sentence pairs in an out-of-
domain training corpus that are in some way sim-

ilar to an in-domain training corpus in terms of
some different metrics. Cross-entropy difference
is a typical ranking function (Moore and Lewis,
2010; Axelrod et al., 2011; Rousseau, 2013;
Schwenk et al., 2012; Mansour et al., 2011).

On the other hand, distributed representations
of words have proliferated during the last years in
the research community. Neural networks provide
powerful tools for processing text, achieving suc-
cess in text classification (Kim, 2014) or in ma-
chine translation (Sutskever et al., 2014; Bahdanau
et al., 2015). Furthermore, (Duh et al., 2013)
leveraged neural language models to perform DS,
reporting substantial gains over conventional n-
gram language model-based DS.

Recently, convolutional neural networks (CNN)
have also been used in data selection (Chen and
Huang, 2016; Chen et al., 2016). In these works,
the authors used a similar strategy to the one pro-
posed in Section 2.3, but in a different scenario:
they have no in-domain training corpus; only a
large out-of-domain pool and small sets of trans-
lation instances. Their goal was to select, from the
out-of-domain corpus, the more suitable samples
for translating their in-domain corpora.

This paper tackles DS by taking advantage of
neural network as sentence classifiers with the ul-
timate goal of obtaining corpus subsets that min-
imize the training corpus size, while improving
translation quality.

The paper is structured as follows. Section 2,
presents our DS method, featuring two differ-
ent neural network architectures: a CNN (Le-
Cun et al., 1998) and a bidirectional long short-
term memory (BLSTM) network (Hochreiter and
Schmidhuber, 1997; Schuster and Paliwal, 1997).
In Section 3, the experimental design and results
are detailed. Finally, the main results of the work
and future work are discussed in Section 4.

ar
X

iv
:1

61
2.

05
55

5v
2 

 [
cs

.C
L

] 
 2

1 
D

ec
 2

01
6


