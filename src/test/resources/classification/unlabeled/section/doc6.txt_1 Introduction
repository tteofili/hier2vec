
In neural language modelling, a neural network estimates a distribution over sequences of
words or characters that belong to a given language (Bengio et al., 2003). In neural machine
translation, the network estimates a distribution over sequences in the target language
conditioned on a given sequence in the source language. In the latter case the network can
be thought of as composed of two sub-networks, a source network that processes the source
sequence into a representation and a target network that uses the representation of the
source to generate the target sequence (Kalchbrenner and Blunsom, 2013)

Recurrent neural networks (RNN) are powerful sequence models (Hochreiter and Schmidhuber,
1997) and are widely used in language modelling (Mikolov et al., 2010), yet they have a
potential drawback. RNNs have an inherently serial structure that prevents them from being
run in parallel along the sequence length. Forward and backward signals in a RNN also need
to traverse the full distance of the serial path to reach from one point to another in the
sequence. The larger the distance the harder it is to learn dependencies between the points
(Hochreiter et al., 2001).

A number of neural architectures have been proposed for modelling translation (Kalchbrenner
and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Kalchbren-
ner et al., 2016a; Kaiser and Bengio, 2016). These networks either have running time that
is super linear in the length of the source and target sequences, or they process the source
sequence into a constant size representation, burdening the model with a memorization step.
Both of these drawbacks grow more severe as the length of the sequences increases.

We present a neural translation model, the ByteNet, and a neural language model, the
ByteNet Decoder, that aim at addressing these drawbacks. The ByteNet uses convolutional
neural networks with dilation for both the source network and the target network. The
ByteNet connects the source and target networks via stacking and unfolds the target network
dynamically to generate variable length output sequences. We view the ByteNet as an
instance of a wider family of sequence-mapping architectures that stack the sub-networks
and use dynamic unfolding. The sub-networks themselves may be convolutional or recurrent.

1

ar
X

iv
:1

61
0.

10
09

9v
1 

 [
cs

.C
L

] 
 3

1 
O

ct
 2

01
6



t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16t10

s0 s1 s2 s3 s4 s5 s6 s7 s8 s9 s10 s11 s12 s13 s14 s15 s16

t11 t12 t13 t14 t15 t16 t17t10t9t8t7t6t5t4t3t2t1

Figure 1: The architecture of the ByteNet. The target network (blue) is stacked on top of
the source network (red). The target network generates the variable-length target sequence
using dynamic unfolding. The ByteNet Decoder is the target network of the ByteNet.

The ByteNet with recurrent sub-networks may be viewed as a strict generalization of the
RNN Enc-Dec network (Sutskever et al., 2014; Cho et al., 2014) (Sect. 4). The ByteNet
Decoder has the same architecture as the target network in the ByteNet. In contrast to neural
language models based on RNNs (Mikolov et al., 2010) or on feed-forward networks (Bengio
et al., 2003; Arisoy et al., 2012), the ByteNet Decoder is based on a novel convolutional
structure designed to capture a very long range of past inputs.

The ByteNet has a number of beneficial computational and learning properties. From a
computational perspective, the network has a running time that is linear in the length of
the source and target sequences (up to a constant c ≈ log d where d is the size of the desired
dependency field). The computation in the source network during training and decoding and
in the target network during training can also be run efficiently in parallel along the strings
– by definition this is not possible for a target network during decoding (Sect. 2). From a
learning perspective, the representation of the source string in the ByteNet is resolution
preserving ; the representation sidesteps the need for memorization and allows for maximal
bandwidth between the source and target networks. In addition, the distance traversed
by forward and backward signals between any input and output tokens in the networks
corresponds to the fixed depth of the networks and is largely independent of the distance
between the tokens. Dependencies over large distances are connected by short paths and can
be learnt more easily.

We deploy ByteNets on raw sequences of characters. We evaluate the ByteNet Decoder on
the Hutter Prize Wikipedia task; the model achieves 1.33 bits/character showing that the
convolutional language model is able to outperform the previous best results obtained with
recurrent neural networks. Furthermore, we evaluate the ByteNet on raw character-level
machine translation on the English-German WMT benchmark. The ByteNet achieves a
score of 18.9 and 21.7 BLEU points on, respectively, the 2014 and the 2015 test sets; these
results approach the best results obtained with other neural translation models that have
quadratic running time (Chung et al., 2016b; Wu et al., 2016a). We use gradient-based
visualization (Simonyan et al., 2013) to reveal the latent structure that arises between the
source and target sequences in the ByteNet. We find the structure to mirror the expected
word alignments between the source and target sequences.
