
We first evaluate the ByteNet Decoder separately on a character-level language modelling
benchmark. We use the Hutter Prize version of the Wikipedia dataset and follow the standard
split where the first 90 million bytes are used for training, the next 5 million bytes are used
for validation and the last 5 million bytes are used for testing (Chung et al., 2015). The
total number of characters in the vocabulary is 205.

6



Model Test

Stacked LSTM (Graves, 2013) 1.67
GF-LSTM (Chung et al., 2015) 1.58
Grid-LSTM (Kalchbrenner et al., 2016a) 1.47
Layer-normalized LSTM (Chung et al., 2016a) 1.46
MI-LSTM (Wu et al., 2016b) 1.44
Recurrent Highway Networks (Srivastava et al., 2015) 1.42
Recurrent Memory Array Structures (Rocki, 2016) 1.40
HM-LSTM (Chung et al., 2016a) 1.40
Layer Norm HyperLSTM (Ha et al., 2016) 1.38
Large Layer Norm HyperLSTM (Ha et al., 2016) 1.34
ByteNet Decoder 1.33

Table 2: Negative log-likelihood results in bits/byte on the Hutter Prize Wikipedia benchmark.

Model WMT Test ’14 WMT Test ’15

Phrase Based MT 20.7(1) 24.0(2)

RNN Enc-Dec 11.3(3)

RNN Enc-Dec + reverse 14.0(3)

RNN Enc-Dec Att 16.9(3)

RNN Enc-Dec Att + deep (Zhou et al., 2016) 20.6

RNN Enc-Dec Att + local p + unk replace 20.9(3)

RNN Enc-Dec Att + BPE in + BPE out 19.98(4) 21.72(4)

RNN Enc-Dec Att + BPE in + char out 21.33(4) 23.45(4)

GNMT + char in + char out (Wu et al., 2016a) 22.8
ByteNet 18.9 21.7

Table 3: BLEU scores on En-De WMT NewsTest 2014 and 2015 test sets. The ByteNet is
character-level. The other models are word-level unless otherwise noted. Result (1) is from
(Freitag et al., 2014), result (2) is from (Williams et al., 2015), results (3) are from (Luong
et al., 2015) and results (4) are from (Chung et al., 2016b)

The ByteNet Decoder that we use for the result has 25 residual blocks split into five sets of
five blocks each; for the five blocks in each set the dilation rates are, respectively, 1,2,4,8 and
16. The masked kernel has size 3. This gives a receptive field of 315 characters. The number
of hidden units d is 892. For this task we use residual multiplicative blocks and Sub-BN
(Fig. 3 Right); we do not use bags of character n-grams for the inputs. For the optimization
we use Adam (Kingma and Ba, 2014) with a learning rate of 10−2 and a weight decay term
of 10−5. We do not reduce the learning rate during training. At each step we sample a batch
of sequences of 515 characters each, use the first 315 characters as context and predict only
the latter 200 characters.

Table 2 lists recent results of various neural sequence models on the Wikipedia dataset. All the
results except for the ByteNet result are obtained using some variant of the LSTM recurrent
neural network (Hochreiter and Schmidhuber, 1997). The ByteNet Decoder achieves 1.33
bits/character on the test set.
