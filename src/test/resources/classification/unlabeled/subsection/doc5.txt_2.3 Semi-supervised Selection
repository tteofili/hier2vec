
Properly training these neural classifiers may be
a challenging task, since the in-domain data is
scarce. Hence, for training them, we follow
a semi-supervised iterative protocol (Yarowsky,
1995).

Since the data selection is a binary classifica-
tion problem, we need a set of positive and nega-
tive training samples. We start from an initial set
of positive samples P0 and a set of negative sam-
ples N0 and train an initial model M0. At each
iteration i ≥ 0, we classify all sentences belong-
ing to the out-of-domain pool (Gi). We extract
a number r of top-scoring sentences and include
them into the set of positive samples, producing
a new set of positive samples Pi+1. Analogously,
the r bottom-scoring sentences are included into



a new negative samples set Ni+1. Hence, at each
iteration, we remove 2r samples from the out-of-
domain set, producing the pool Gi+1. Then, we
start a new iteration, training a new model Mi+1
with {Pi+1 ∪ Ni+1}. This is repeated until there
are no more sentences in the out-of-domain pool.

We set our in-domain corpus I as P0. We ran-
domly extract |I| sentences from G for construct-
ing N0. The initial out-of-domain pool G0 is de-
fined as {G−N0}.
