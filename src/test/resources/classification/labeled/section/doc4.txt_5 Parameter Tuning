
Joshua 4 included the PRO tuner. Joshua 6 adds two new large-scale discriminative
decoders: k-best batch MIRA (Crammer et al., 2006; Cherry and Foster, 2012) and
AdaGrad (Duchi et al., 2011; Green et al., 2012). The usages of these tuners (as well as
Z-MERT, which has always been a part of Joshua) are consistent, except for the class
names and a few lines specifying the parameters in the configuration files.

A difficulty with decoding with large feature sets is that the set of observed fea-
tures is not known prior to tuning. Joshuaâ€™s discriminative tuners do not make any
distinction between dense and sparse features, and will incorporate newly-fired fea-
tures into their learning procedures, as those features are generated and encountered
during the tuning process.

3e.g., languagemodels are feature functions returning a state object representing the target-side context.

9



PBML 104 OCTOBER 2015
