
We train 100k-vocabulary word embeddings using

curriculum training (Turian et al., 2010) with Equa-

tion 5. For each curriculum, we sort the vocabu-

lary by frequency and segment the vocabulary by a

band-size taken from {5k, 10k, 25k, 50k}. Separate
bands of the vocabulary are trained in parallel using

minibatch L-BFGS on the Chinese Gigaword cor-

pus 3. We train 100,000 iterations for each curricu-

lum, and the entire 100k vocabulary is trained for

500,000 iterations. The process takes approximately

19 days on a eight-core machine. We show visual-

ization of learned embeddings overlaid with English

in Figure 1. The two-dimensional vectors for this vi-

sualization is obtained with t-SNE (van der Maaten

and Hinton, 2008). To make the figure comprehen-

sible, subsets of Chinese words are provided with

reference translations in boxes with green borders.

Words across the two languages are positioned by

the semantic relationships implied by their embed-

dings.

Figure 1: Overlaid bilingual embeddings: English words

are plotted in yellow boxes, and Chinese words in green;

reference translations to English are provided in boxes

with green borders directly below the original word.
