Encoder–Decoder

Here we propose to train the RNN Encoder–
Decoder (see Sec. 2.2) on a table of phrase pairs
and use its scores as additional features in the log-
linear model in Eq. (9) when tuning the SMT de-
coder.

When we train the RNN Encoder–Decoder, we
ignore the (normalized) frequencies of each phrase
pair in the original corpora. This measure was
taken in order (1) to reduce the computational ex-
pense of randomly selecting phrase pairs from a
large phrase table according to the normalized fre-
quencies and (2) to ensure that the RNN Encoder–
Decoder does not simply learn to rank the phrase
pairs according to their numbers of occurrences.
One underlying reason for this choice was that the
existing translation probability in the phrase ta-
ble already reflects the frequencies of the phrase

2 Without loss of generality, from here on, we refer to
p(e | f) for each phrase pair as a translation model as well

pairs in the original corpus. With a fixed capacity
of the RNN Encoder–Decoder, we try to ensure
that most of the capacity of the model is focused
toward learning linguistic regularities, i.e., distin-
guishing between plausible and implausible trans-
lations, or learning the “manifold” (region of prob-
ability concentration) of plausible translations.

Once the RNN Encoder–Decoder is trained, we
add a new score for each phrase pair to the exist-
ing phrase table. This allows the new scores to en-
ter into the existing tuning algorithm with minimal
additional overhead in computation.

As Schwenk pointed out in (Schwenk, 2012),
it is possible to completely replace the existing
phrase table with the proposed RNN Encoder–
Decoder. In that case, for a given source phrase,
the RNN Encoder–Decoder will need to generate
a list of (good) target phrases. This requires, how-
ever, an expensive sampling procedure to be per-
formed repeatedly. In this paper, thus, we only
consider rescoring the phrase pairs in the phrase
table.
